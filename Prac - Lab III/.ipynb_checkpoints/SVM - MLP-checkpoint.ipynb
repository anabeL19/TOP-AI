{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "\n",
    "def Leer_Datos(filename,atrributes):\n",
    "    data = pd.read_csv(filename, usecols=atrributes) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = data.mean(axis=0)\n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X, y, W):\n",
    "    \"\"\" accerted values respect to predict\"\"\"\n",
    "    accerted = 0.0\n",
    "    num = 5\n",
    "    for i in range(0, num): #place to 5 can be X.shape[0]\n",
    "        temp = list(W.values())[len(W)-1]\n",
    "        temp = int(np.round_(temp[i]))\n",
    "        if temp == y[i]:\n",
    "            accerted += 1\n",
    "    return accerted/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y(data, name_col):\n",
    "    X = data.loc[:, data.columns != name_col]\n",
    "    y = data[name_col]\n",
    "    return X.values, y.values\n",
    "\n",
    "def PrepareXandY(training,test):\n",
    "    train_X = training[:,:-1]\n",
    "    train_X = np.concatenate((np.ones([train_X.shape[0], 1]), train_X), axis=1)\n",
    "\n",
    "    test_X = test[:,:-1]\n",
    "    test_X = np.concatenate((np.ones([test_X.shape[0], 1]), test_X), axis=1)\n",
    "\n",
    "    train_y = training[:,-1]\n",
    "    test_y = test[:,-1]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    col_class = data[:,-1]\n",
    "    #col_class = np.array([0,0,1,0,1,0,1,2,2,2,0])\n",
    "    num_rows = col_class.shape[0]\n",
    "    unique, counts = np.unique(col_class, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "   \n",
    "    percent_per_class = {}\n",
    "\n",
    "    classes = []\n",
    "    for key in distribution:\n",
    "        percent_per_class[key]=round((distribution[key] * 100) /  num_rows)\n",
    "        classes.append(key)\n",
    "   \n",
    "    num_per_fold = round(num_rows/k)\n",
    "\n",
    "    num_samples_by_class_per_fold = {}\n",
    "\n",
    "    for key in percent_per_class:\n",
    "        num_samples_by_class_per_fold[key]=round((num_per_fold * percent_per_class[key])/100)\n",
    "    \n",
    "\n",
    "    list_indices=[]\n",
    "    count_classes={}\n",
    "\n",
    "    for key in distribution:\n",
    "        list_class = []\n",
    "        for i in range(k):\n",
    "            list_class.append(0)\n",
    "        count_classes[key]=list_class\n",
    "    \n",
    "    for i in range(k):\n",
    "        list_indices.append([])\n",
    "\n",
    "    extra = 0\n",
    "    for i in range(num_rows):\n",
    "        added = False\n",
    "        for key in distribution:\n",
    "            if col_class[i] == key:\n",
    "                for j in range(k):\n",
    "                    if(count_classes[key][j]<num_samples_by_class_per_fold[key]):\n",
    "                        count_classes[key][j]+=1\n",
    "                        #print(list_indices[j],list_indices[j].count(0))\n",
    "                        list_indices[j].append(i)\n",
    "                        added = True\n",
    "                        break\n",
    "                if added:\n",
    "                    break\n",
    "        if not added:\n",
    "            list_indices[extra].append(i)\n",
    "            extra = (extra+1)%k\n",
    "    \n",
    "    return data, list_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfolds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    size_fold = int(data.shape[0] / k)\n",
    "    _sz_fold = int(data.shape[0] % k)\n",
    "    data = data[:data.shape[0]-_sz_fold,:]\n",
    "    kfolds = []\n",
    "    idx_row = 0\n",
    "    for i in range(k):\n",
    "        X, y = X_y(data[idx_row:idx_row+size_fold, :])\n",
    "        kfolds.append({\"X\": X, \"y\" : y})\n",
    "        idx_row += size_fold\n",
    "    return kfolds, size_fold\n",
    "\n",
    "def kfolds_cross_validation(data, k):\n",
    "    for i in range(0, k):\n",
    "        temp_test_data = k_folds[i]\n",
    "        temp_train_data = np.delete(k_folds, i, axis=0)\n",
    "        temp_train_data = temp_train_data.reshape(-1, temp_test_data.shape[1])\n",
    "        np.random.shuffle(temp_train_data)\n",
    "\n",
    "        x_train_set, y_train_set = get_x_y_data(temp_train_data)\n",
    "        y_train_set = y_train_set.reshape(y_train_set.shape[0])\n",
    "\n",
    "        x_test_set, y_test_set = get_x_y_data(temp_test_data)\n",
    "        y_test_set = y_test_set.reshape(y_test_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_function(X, y, W):\n",
    "    m = X.shape[0]\n",
    "    pred = sigmoid(X)\n",
    "    print('pred', pred)\n",
    "    cross_entropy = np.sum( (y * np.log(pred)) + ((1-y) * np.log(1-pred)) )\n",
    "    #print(\"1: \",np.dot(y.T, np.log(pred)))\n",
    "    #print(\"2: \", np.dot((1-y).T, np.log(1-pred)))\n",
    "    return (-1/m) * cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dS(D):\n",
    "    return sigmoid(D)*(1.0-sigmoid(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    A = {}\n",
    "    i=0\n",
    "    for key, value in W.items():\n",
    "        sumt = np.matmul(X, value)\n",
    "        y = sigmoid(sumt)\n",
    "        A.setdefault(i, y)\n",
    "        #other way: A['activation'+str(i)]=y\n",
    "        i = i+1\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, A, y, W, learning_rate):\n",
    "    newW = {}\n",
    "    \n",
    "    #last layer\n",
    "    last_output = (list(A.values())[len(A)-1])\n",
    "    real_error = y - last_output\n",
    "    #print('real error: ', real_error)\n",
    "    #delta = last_output * (1-last_output) * (1-last_output)\n",
    "    delta = real_error * dS(1-last_output) \n",
    "    \n",
    "    for key, value in W.items():\n",
    "        #print('val: ', value)\n",
    "        #print('val', value.shape[0], value.shape[1])\n",
    "        mult = learning_rate * A[key] * delta\n",
    "        result = value + mult\n",
    "        #print(result)\n",
    "        newW.setdefault(key, result)\n",
    "    return newW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, W):\n",
    "    m = y.shape[0]  \n",
    "    return (1/m) * np.matmul(X.T, sigmoid(X, W.values()) - y)\n",
    "\n",
    "def gradient_descent( X, y, W, nb_iterations, learning_rate):\n",
    "    cost_history = np.zeros(nb_iterations)\n",
    "    for i in range(nb_iterations):\n",
    "        error_epoch = 0.0\n",
    "        A = {}\n",
    "        for j in range(0, X.shape[0]):\n",
    "                A = forward(X[j], W)\n",
    "                last_errors = np.divide(np.power(y - list(W.values())[len(W)-1], 2), 2)\n",
    "                error_epoch += np.sum(last_errors)\n",
    "                backward(X[j], A, y[j], W, learning_rate)\n",
    "        #cost_history[i] = calculate_cost_function( X, y, W)\n",
    "    return W, cost_history\n",
    "                \n",
    "def _gradient_descent( X, y, W, nb_iterations, learning_rate):\n",
    "    '''Return the final theta vector and array of cost history over nro of iterations\n",
    "    nb_iterations: or epochs\n",
    "    '''\n",
    "    m = y.shape[0]\n",
    "    cost_history = np.zeros(nb_iterations)\n",
    "    A = {}\n",
    "    for i in range(nb_iterations):\n",
    "        #prediction = X.dot(W.values())\n",
    "        \n",
    "        print('W.values()', W.values()[i])\n",
    "        prediction = np.matmul(X,W.values()[i])\n",
    "        print('prediction ',prediction.shape)\n",
    "        print('y ', y.shape)\n",
    "        \n",
    "        if W.values()[len(W)-1]:\n",
    "            prediction = np.matmul(X,W.values()[i])\n",
    "            print(prediction - y)\n",
    "            ro = (X.dot(prediction - y))\n",
    "            print('ro ', ro)\n",
    "            \n",
    "            r = ((1/m) * learning_rate * (X.T.dot(prediction - y)))\n",
    "        print('r ', r)\n",
    "        W = W.values() - r\n",
    "        #cost_history[i] = calculate_cost_function( X, y, W) \n",
    "    return W, cost_history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optional_create_training_test(data):\n",
    "    np.random.shuffle(data)\n",
    "    col = data.shape[1]-1\n",
    "    k = len(data)\n",
    "    X_train = data[:int((60* k) / 100), :col]\n",
    "    y_train = data[:int((60* k) / 100), col]\n",
    "    X_test = data[int((60* k) / 100):, :col]\n",
    "    y_test = data[int((60* k) / 100):, col]\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_training_test(data):\n",
    "    num_rows = data.shape[0]\n",
    "    train_percentage = 0.6\n",
    "    row_split_data = int(num_rows * train_percentage)\n",
    "    training, test = data[:row_split_data, :], data[row_split_data:, :]\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_W(X, nb_neuron):\n",
    "    return np.random.rand(X.shape[1], nb_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def AccSVMfold(fold,model):\n",
    "    model.fit(fold[0], fold[1])\n",
    "    pred_linear = model.predict(fold[2])\n",
    "    return accuracy_score(fold[3],pred_linear)\n",
    "\n",
    "def getAvgAccSMV(folds,svm_reg):\n",
    "    accuracy = AccSVMfold(folds[0],svm_reg)\n",
    "    accuracy1 = AccSVMfold(folds[1],svm_reg)\n",
    "    accuracy2 = AccSVMfold(folds[2],svm_reg)\n",
    "\n",
    "    return (accuracy + accuracy1 + accuracy2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTitanicData():\n",
    "    files = [(\"../titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"../gender_submission.csv\",[\"Survived\"]),\n",
    "              (\"../titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"])]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "\n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = data.replace(to_replace='female',value=1,regex=True)\n",
    "    data = data.replace(to_replace='male',value=0,regex=True)\n",
    "    data = data.replace(to_replace='C',value=0,regex=True)\n",
    "    data = data.replace(to_replace='S',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "\n",
    "    f_name = files[1][0]\n",
    "    atrributes = files[1][1]\n",
    "    data2 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = np.concatenate((data, data2), axis=1)\n",
    "\n",
    "    f_name = files[2][0]\n",
    "    atrributes = files[2][1]\n",
    "    data3 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data3 = data3[[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]]\n",
    "\n",
    "    data3 = data3.replace(to_replace='female',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='male',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='C',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='S',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data[:,1:3] = Normalizar_Datos(data[:,1:3])\n",
    "    return data\n",
    "\n",
    "def getIrisData():\n",
    "    files = [(\"../Iris.csv\",[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\"])]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "    \n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='Iris-setosa',value=0,regex=True)\n",
    "    data = data.replace(to_replace='Iris-versicolor',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Iris-virginica',value=2,regex=True)\n",
    "\n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data = data.values\n",
    "    data[:,0:4] = Normalizar_Datos(data[:,0:4])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'k' referenced before assignment",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-582279a6e455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mpdObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HL|N'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mexperimentI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-582279a6e455>\u001b[0m in \u001b[0;36mexperimentI\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexperimentI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetTitanicData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_k_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfold1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'k' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def experimentI():\n",
    "    data = GetTitanicData()\n",
    "    k = 3\n",
    "    data, indices = create_k_folds(data, k)\n",
    "    \n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 100 #nb_iterations\n",
    "    hidde_layers = [1, 2, 3]\n",
    "    nb_neurons = [5, 6, 7]\n",
    "    \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "    \n",
    "    result_tb = [hidde_layers]\n",
    "    for layer in hidde_layers:\n",
    "            W1 = {}\n",
    "            W2 = {}\n",
    "            W3 = {}\n",
    "            hlayers = []\n",
    "            for nb_neuron in nb_neurons:\n",
    "                accuracy_test_total = 0.0\n",
    "                countw = 0\n",
    "                \n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                for i in range(layer):\n",
    "                        W1.setdefault(countw, create_W(train_X1, nb_neuron))\n",
    "                        W2.setdefault(countw, create_W(train_X2, nb_neuron))\n",
    "                        W3.setdefault(countw, create_W(train_X3, nb_neuron))\n",
    "                        countw = countw + 1\n",
    "                #capa salida\n",
    "                W1.setdefault(countw, create_W(train_X1, 1))\n",
    "                W2.setdefault(countw, create_W(train_X2, 1))\n",
    "                W3.setdefault(countw, create_W(train_X3, 1))\n",
    "                countw = countw + 1\n",
    "                W1, cost_history1 = gradient_descent(train_X1, train_y1, W1, epochs, learning_rate)\n",
    "                W2, cost_history1 = gradient_descent(train_X2, train_y2, W2, epochs, learning_rate)\n",
    "                W3, cost_history1 = gradient_descent(train_X3, train_y3, W3, epochs, learning_rate)\n",
    "                \n",
    "                accuracy_test1 = calculate_accuracy(test_X1, test_y1, W1)\n",
    "                accuracy_test2 = calculate_accuracy(test_X2, test_y2, W2)\n",
    "                accuracy_test3 = calculate_accuracy(test_X3, test_y3, W3)\n",
    "                \n",
    "                accuracy_test_total = (accuracy_test1 + accuracy_test2 + accuracy_test3)/k\n",
    "                hlayers.append(\"%.4f\" % accuracy_test_total)\n",
    "            result_tb.append(hlayers)    \n",
    "    m = np.asarray(result_tb)\n",
    "    pdObj = pd.DataFrame(m.T[:], columns=['HL|N','5','6','7']) \n",
    "    print(pdObj)\n",
    "experimentI()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['1' '2' '3']\n ['0.3333' '0.3333' '0.3333']\n ['0.2667' '0.2667' '0.2667']\n ['0.4000' '0.4000' '0.4000']]\n  HL|N       5       6       7\n0    1  0.3333  0.2667  0.4000\n1    2  0.3333  0.2667  0.4000\n2    3  0.3333  0.2667  0.4000\n"
     ]
    }
   ],
   "source": [
    "def experimentI():\n",
    "    k = 3\n",
    "    data=getIrisData()\n",
    "    \n",
    "    data, indices = create_k_folds(data, k)\n",
    "\n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    epochs = 100 #nb_iterations\n",
    "    hidde_layers = [1, 2, 3]\n",
    "    nb_neurons = [5, 6, 7]\n",
    "    k = 3\n",
    "    \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "    \n",
    "    result_tb = [hidde_layers]\n",
    "    for layer in hidde_layers:\n",
    "            W1 = {}\n",
    "            W2 = {}\n",
    "            W3 = {}\n",
    "            hlayers = []\n",
    "            for nb_neuron in nb_neurons:\n",
    "                accuracy_test_total = 0.0\n",
    "                countw = 0\n",
    "                \n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                for i in range(layer):\n",
    "                        W1.setdefault(countw, create_W(train_X1, nb_neuron))\n",
    "                        W2.setdefault(countw, create_W(train_X2, nb_neuron))\n",
    "                        W3.setdefault(countw, create_W(train_X3, nb_neuron))\n",
    "                        countw = countw + 1\n",
    "                #capa salida\n",
    "                W1.setdefault(countw, create_W(train_X1, 1))\n",
    "                W2.setdefault(countw, create_W(train_X2, 1))\n",
    "                W3.setdefault(countw, create_W(train_X3, 1))\n",
    "                countw = countw + 1\n",
    "                W1, cost_history1 = gradient_descent(train_X1, train_y1, W1, epochs, learning_rate)\n",
    "                W2, cost_history1 = gradient_descent(train_X2, train_y2, W2, epochs, learning_rate)\n",
    "                W3, cost_history1 = gradient_descent(train_X3, train_y3, W3, epochs, learning_rate)\n",
    "                \n",
    "                accuracy_test1 = calculate_accuracy(test_X1, test_y1, W1)\n",
    "                accuracy_test2 = calculate_accuracy(test_X2, test_y2, W2)\n",
    "                accuracy_test3 = calculate_accuracy(test_X3, test_y3, W3)\n",
    "                \n",
    "                accuracy_test_total = (accuracy_test1 + accuracy_test2 + accuracy_test3)/k\n",
    "                hlayers.append(\"%.4f\" % accuracy_test_total)\n",
    "            result_tb.append(hlayers)    \n",
    "    m = np.asarray(result_tb)\n",
    "    pdObj = pd.DataFrame(m.T[:], columns=['HL|N','5','6','7']) \n",
    "    print(pdObj)\n",
    "experimentI()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TITANIC\n",
      "[[  1.           0.84951331   0.84951605   0.83896579]\n",
      " [  3.           0.84951331   0.84951605   0.83319373]\n",
      " [  5.           0.84951331   0.84951605   0.83126969]\n",
      " [100.           0.84951331   0.84951605   0.81402258]]\n",
      "IRIS\n",
      "[[  1.           0.96         0.96         0.96666667]\n",
      " [  3.           0.96666667   0.94666667   0.96666667]\n",
      " [  5.           0.97333333   0.94666667   0.96      ]\n",
      " [100.           0.95333333   0.94         0.94666667]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def experimentII(function_data):\n",
    "    k = 3\n",
    "    data=function_data()\n",
    "    data, indices = create_k_folds(data, k)\n",
    "    \n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "\n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "\n",
    "    folds = [(train_X1, train_y1, test_X1, test_y1),\n",
    "    (train_X2, train_y2, test_X2, test_y2),\n",
    "    (train_X3, train_y3, test_X3, test_y3)]\n",
    "\n",
    "    kind_kernel = [\"linear\", \"poly\", \"rbf\"]\n",
    "    setC = [1,3,5,100]\n",
    "   \n",
    "    m=[]\n",
    "    for C in setC:\n",
    "        row = [C]\n",
    "\n",
    "        svm_reg_l = SVC(kernel='linear',C=C,gamma=1, decision_function_shape='ovo')\n",
    "        avg_acc_linear = getAvgAccSMV(folds,svm_reg_l)\n",
    "\n",
    "        row.append(avg_acc_linear)\n",
    "            \n",
    "        \n",
    "        svm_reg_p = SVC(kernel='poly',C=C,degree=3,gamma=1,decision_function_shape='ovo')\n",
    "        avg_acc_poly = getAvgAccSMV(folds,svm_reg_p)\n",
    "\n",
    "        row.append(avg_acc_poly)\n",
    "            \n",
    "        svm_reg_g = SVC(kernel='rbf', C=C, gamma=1, decision_function_shape='ovo')\n",
    "        avg_acc_gauss = getAvgAccSMV(folds,svm_reg_g) \n",
    "            \n",
    "        row.append(avg_acc_gauss)\n",
    "        m.append(row)\n",
    "\n",
    "    m = np.asarray(m)\n",
    "    #pdObj = pd.DataFrame(m.T[:], columns=['num','C','linear','poly','gauss']) \n",
    "    print(m)\n",
    "    \n",
    "print(\"TITANIC\")               \n",
    "experimentII(GetTitanicData)\n",
    "\n",
    "print(\"IRIS\")               \n",
    "experimentII(getIrisData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}