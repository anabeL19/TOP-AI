{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "\n",
    "def Leer_Datos(filename,atrributes):\n",
    "    data = pd.read_csv(filename, usecols=atrributes) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = data.mean(axis=0)\n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/ (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X, y, W):\n",
    "    \"\"\" accerted values respect to predict\"\"\"\n",
    "    accerted = 0.0\n",
    "    num = 5\n",
    "    for i in range(0, num): #place to 5 can be X.shape[0]\n",
    "        temp = list(W.values())[len(W)-1]\n",
    "        temp = int(np.round_(temp[i]))\n",
    "        if temp == y[i]:\n",
    "            accerted += 1\n",
    "    return accerted/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y(data, name_col):\n",
    "    X = data.loc[:, data.columns != name_col]\n",
    "    y = data[name_col]\n",
    "    return X.values, y.values\n",
    "\n",
    "def PrepareXandY(training,test):\n",
    "    train_X = training[:,:-1]\n",
    "    train_X = np.concatenate((np.ones([train_X.shape[0], 1]), train_X), axis=1)\n",
    "\n",
    "    test_X = test[:,:-1]\n",
    "    test_X = np.concatenate((np.ones([test_X.shape[0], 1]), test_X), axis=1)\n",
    "\n",
    "    train_y = training[:,-1]\n",
    "    test_y = test[:,-1]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    col_class = data[:,-1]\n",
    "    #col_class = np.array([0,0,1,0,1,0,1,0,0,0])\n",
    "    num_rows = col_class.shape[0]\n",
    "    unique, counts = np.unique(col_class, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    #print(distribution)\n",
    "    percent_of_first_class = round((distribution[0.0] * 100) /  num_rows)\n",
    "   \n",
    "    percent_of_second_class = round((distribution[1.0] * 100) / num_rows)\n",
    "   \n",
    "    num_per_fold = round(num_rows/k)\n",
    "    \n",
    "    num_first_class_per_fold = round((num_per_fold * percent_of_first_class)/100)\n",
    "    num_second_class_per_fold = round((num_per_fold * percent_of_second_class)/100)\n",
    "\n",
    "    list_indices=[]\n",
    "    num_0s=[]\n",
    "    num_1s=[]\n",
    "    \n",
    "    for i in range(k):\n",
    "        list_indices.append([])\n",
    "        num_0s.append(0)\n",
    "        num_1s.append(0)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "    \n",
    "        if col_class[i] == 0.0:\n",
    "            for j in range(k):\n",
    "                if(num_0s[j]<num_first_class_per_fold):\n",
    "                    num_0s[j]+=1\n",
    "                    #print(list_indices[j],list_indices[j].count(0))\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for j in range(k):\n",
    "                if(num_1s[j]<num_second_class_per_fold):\n",
    "                    num_1s[j]+=1\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "    if num_rows%k!=0:\n",
    "        list_indices[k-1].append(col_class.shape[0]-1)\n",
    "    \n",
    "    return list_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfolds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    size_fold = int(data.shape[0] / k)\n",
    "    _sz_fold = int(data.shape[0] % k)\n",
    "    data = data[:data.shape[0]-_sz_fold,:]\n",
    "    kfolds = []\n",
    "    idx_row = 0\n",
    "    for i in range(k):\n",
    "        X, y = X_y(data[idx_row:idx_row+size_fold, :])\n",
    "        kfolds.append({\"X\": X, \"y\" : y})\n",
    "        idx_row += size_fold\n",
    "    return kfolds, size_fold\n",
    "\n",
    "def kfolds_cross_validation(data, k):\n",
    "    for i in range(0, k):\n",
    "        temp_test_data = k_folds[i]\n",
    "        temp_train_data = np.delete(k_folds, i, axis=0)\n",
    "        temp_train_data = temp_train_data.reshape(-1, temp_test_data.shape[1])\n",
    "        np.random.shuffle(temp_train_data)\n",
    "\n",
    "        x_train_set, y_train_set = get_x_y_data(temp_train_data)\n",
    "        y_train_set = y_train_set.reshape(y_train_set.shape[0])\n",
    "\n",
    "        x_test_set, y_test_set = get_x_y_data(temp_test_data)\n",
    "        y_test_set = y_test_set.reshape(y_test_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_function(X, y, W):\n",
    "    m = X.shape[0]\n",
    "    pred = sigmoid(X)\n",
    "    print('pred', pred)\n",
    "    cross_entropy = np.sum( (y * np.log(pred)) + ((1-y) * np.log(1-pred)) )\n",
    "    #print(\"1: \",np.dot(y.T, np.log(pred)))\n",
    "    #print(\"2: \", np.dot((1-y).T, np.log(1-pred)))\n",
    "    return (-1/m) * cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dS(D):\n",
    "    return sigmoid(D)*(1.0-sigmoid(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    A = {}\n",
    "    i=0\n",
    "    for key, value in W.items():\n",
    "        sumt = np.matmul(X, value)\n",
    "        y = sigmoid(sumt)\n",
    "        A.setdefault(i, y)\n",
    "        #other way: A['activation'+str(i)]=y\n",
    "        i = i+1\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, A, y, W, learning_rate):\n",
    "    newW = {}\n",
    "    \n",
    "    #last layer\n",
    "    last_output = (list(A.values())[len(A)-1])\n",
    "    real_error = y - last_output\n",
    "    #print('real error: ', real_error)\n",
    "    #delta = last_output * (1-last_output) * (1-last_output)\n",
    "    delta = real_error * dS(1-last_output) \n",
    "    \n",
    "    for key, value in W.items():\n",
    "        #print('val: ', value)\n",
    "        #print('val', value.shape[0], value.shape[1])\n",
    "        mult = learning_rate * A[key] * delta\n",
    "        result = value + mult\n",
    "        #print(result)\n",
    "        newW.setdefault(key, result)\n",
    "    return newW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, W):\n",
    "    m = y.shape[0]  \n",
    "    return (1/m) * np.matmul(X.T, sigmoid(X, W.values()) - y)\n",
    "\n",
    "def gradient_descent( X, y, W, nb_iterations, learning_rate):\n",
    "    cost_history = np.zeros(nb_iterations)\n",
    "    for i in range(nb_iterations):\n",
    "        error_epoch = 0.0\n",
    "        A = {}\n",
    "        for j in range(0, X.shape[0]):\n",
    "                A = forward(X[j], W)\n",
    "                last_errors = np.divide(np.power(y - list(W.values())[len(W)-1], 2), 2)\n",
    "                error_epoch += np.sum(last_errors)\n",
    "                backward(X[j], A, y[j], W, learning_rate)\n",
    "        #cost_history[i] = calculate_cost_function( X, y, W)\n",
    "    return W, cost_history\n",
    "                \n",
    "def _gradient_descent( X, y, W, nb_iterations, learning_rate):\n",
    "    '''Return the final theta vector and array of cost history over nro of iterations\n",
    "    nb_iterations: or epochs\n",
    "    '''\n",
    "    m = y.shape[0]\n",
    "    cost_history = np.zeros(nb_iterations)\n",
    "    A = {}\n",
    "    for i in range(nb_iterations):\n",
    "        #prediction = X.dot(W.values())\n",
    "        \n",
    "        print('W.values()', W.values()[i])\n",
    "        prediction = np.matmul(X,W.values()[i])\n",
    "        print('prediction ',prediction.shape)\n",
    "        print('y ', y.shape)\n",
    "        \n",
    "        if W.values()[len(W)-1]:\n",
    "            prediction = np.matmul(X,W.values()[i])\n",
    "            print(prediction - y)\n",
    "            ro = (X.dot(prediction - y))\n",
    "            print('ro ', ro)\n",
    "            \n",
    "            r = ((1/m) * learning_rate * (X.T.dot(prediction - y)))\n",
    "        print('r ', r)\n",
    "        W = W.values() - r\n",
    "        #cost_history[i] = calculate_cost_function( X, y, W) \n",
    "    return W, cost_history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optional_create_training_test(data):\n",
    "    np.random.shuffle(data)\n",
    "    col = data.shape[1]-1\n",
    "    k = len(data)\n",
    "    X_train = data[:int((60* k) / 100), :col]\n",
    "    y_train = data[:int((60* k) / 100), col]\n",
    "    X_test = data[int((60* k) / 100):, :col]\n",
    "    y_test = data[int((60* k) / 100):, col]\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_training_test(data):\n",
    "    num_rows = data.shape[0]\n",
    "    train_percentage = 0.6\n",
    "    row_split_data = int(num_rows * train_percentage)\n",
    "    training, test = data[:row_split_data, :], data[row_split_data:, :]\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_W(X, nb_neuron):\n",
    "    return np.random.rand(X.shape[1], nb_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  HL|N       5       6       7\n",
      "0    1  0.5333  0.5333  0.4667\n",
      "1    2  0.5333  0.5333  0.4667\n",
      "2    3  0.5333  0.5333  0.4667\n"
     ]
    }
   ],
   "source": [
    "def experimentI():\n",
    "    files = [(\"titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"gender_submission.csv\",[\"Survived\"]),\n",
    "              (\"titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"])]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "    k = 3\n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='female',value=1,regex=True)\n",
    "    data = data.replace(to_replace='male',value=0,regex=True)\n",
    "    data = data.replace(to_replace='C',value=0,regex=True)\n",
    "    data = data.replace(to_replace='S',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    f_name = files[1][0]\n",
    "    atrributes = files[1][1]\n",
    "    data2 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = np.concatenate((data, data2), axis=1)\n",
    "\n",
    "    f_name = files[2][0]\n",
    "    atrributes = files[2][1]\n",
    "    data3 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data3 = data3[[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]]\n",
    "\n",
    "    data3 = data3.replace(to_replace='female',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='male',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='C',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='S',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data[:,1:3] = Normalizar_Datos(data[:,1:3])\n",
    "    indices = create_k_folds(data, k)\n",
    "    \n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 100 #nb_iterations\n",
    "    hidde_layers = [1, 2, 3]\n",
    "    nb_neurons = [5, 6, 7]\n",
    "    k = 3\n",
    "    \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "    \n",
    "    result_tb = [hidde_layers]\n",
    "    for layer in hidde_layers:\n",
    "            W1 = {}\n",
    "            W2 = {}\n",
    "            W3 = {}\n",
    "            hlayers = []\n",
    "            for nb_neuron in nb_neurons:\n",
    "                accuracy_test_total = 0.0\n",
    "                countw = 0\n",
    "                \n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                for i in range(layer):\n",
    "                        W1.setdefault(countw, create_W(train_X1, nb_neuron))\n",
    "                        W2.setdefault(countw, create_W(train_X2, nb_neuron))\n",
    "                        W3.setdefault(countw, create_W(train_X3, nb_neuron))\n",
    "                        countw = countw + 1\n",
    "                #capa salida\n",
    "                W1.setdefault(countw, create_W(train_X1, 1))\n",
    "                W2.setdefault(countw, create_W(train_X2, 1))\n",
    "                W3.setdefault(countw, create_W(train_X3, 1))\n",
    "                countw = countw + 1\n",
    "                W1, cost_history1 = gradient_descent(train_X1, train_y1, W1, epochs, learning_rate)\n",
    "                W2, cost_history1 = gradient_descent(train_X2, train_y2, W2, epochs, learning_rate)\n",
    "                W3, cost_history1 = gradient_descent(train_X3, train_y3, W3, epochs, learning_rate)\n",
    "                \n",
    "                accuracy_test1 = calculate_accuracy(test_X1, test_y1, W1)\n",
    "                accuracy_test2 = calculate_accuracy(test_X2, test_y2, W2)\n",
    "                accuracy_test3 = calculate_accuracy(test_X3, test_y3, W3)\n",
    "                \n",
    "                accuracy_test_total = (accuracy_test1 + accuracy_test2 + accuracy_test3)/k\n",
    "                hlayers.append(\"%.4f\" % accuracy_test_total)\n",
    "            result_tb.append(hlayers)    \n",
    "    m = np.asarray(result_tb)\n",
    "    pdObj = pd.DataFrame(m.T[:], columns=['HL|N','5','6','7']) \n",
    "    print(pdObj)\n",
    "experimentI()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  HL|N       5       6       7\n",
      "0    1  0.3333  0.2667  0.2667\n",
      "1    2  0.3333  0.2667  0.2667\n",
      "2    3  0.3333  0.2667  0.2667\n"
     ]
    }
   ],
   "source": [
    "def experimentI():\n",
    "    files = [(\"Iris.csv\",[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\"])]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "    k = 3\n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='Iris-setosa',value=0,regex=True)\n",
    "    data = data.replace(to_replace='Iris-versicolor',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Iris-virginica',value=2,regex=True)\n",
    "\n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data = data.values\n",
    "    data[:,0:4] = Normalizar_Datos(data[:,0:4])\n",
    "    \n",
    "    indices = create_k_folds(data, k)\n",
    "\n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    epochs = 100 #nb_iterations\n",
    "    hidde_layers = [1, 2, 3]\n",
    "    nb_neurons = [5, 6, 7]\n",
    "    k = 3\n",
    "    \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "    \n",
    "    result_tb = [hidde_layers]\n",
    "    for layer in hidde_layers:\n",
    "            W1 = {}\n",
    "            W2 = {}\n",
    "            W3 = {}\n",
    "            hlayers = []\n",
    "            for nb_neuron in nb_neurons:\n",
    "                accuracy_test_total = 0.0\n",
    "                countw = 0\n",
    "                \n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                for i in range(layer):\n",
    "                        W1.setdefault(countw, create_W(train_X1, nb_neuron))\n",
    "                        W2.setdefault(countw, create_W(train_X2, nb_neuron))\n",
    "                        W3.setdefault(countw, create_W(train_X3, nb_neuron))\n",
    "                        countw = countw + 1\n",
    "                #capa salida\n",
    "                W1.setdefault(countw, create_W(train_X1, 1))\n",
    "                W2.setdefault(countw, create_W(train_X2, 1))\n",
    "                W3.setdefault(countw, create_W(train_X3, 1))\n",
    "                countw = countw + 1\n",
    "                W1, cost_history1 = gradient_descent(train_X1, train_y1, W1, epochs, learning_rate)\n",
    "                W2, cost_history1 = gradient_descent(train_X2, train_y2, W2, epochs, learning_rate)\n",
    "                W3, cost_history1 = gradient_descent(train_X3, train_y3, W3, epochs, learning_rate)\n",
    "                \n",
    "                accuracy_test1 = calculate_accuracy(test_X1, test_y1, W1)\n",
    "                accuracy_test2 = calculate_accuracy(test_X2, test_y2, W2)\n",
    "                accuracy_test3 = calculate_accuracy(test_X3, test_y3, W3)\n",
    "                \n",
    "                accuracy_test_total = (accuracy_test1 + accuracy_test2 + accuracy_test3)/k\n",
    "                hlayers.append(\"%.4f\" % accuracy_test_total)\n",
    "            result_tb.append(hlayers)    \n",
    "    m = np.asarray(result_tb)\n",
    "    pdObj = pd.DataFrame(m.T[:], columns=['HL|N','5','6','7']) \n",
    "    print(pdObj)\n",
    "experimentI()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Archivo: ', 'data/heart.csv')\n",
      "  C|kernel  linear    poly     rbf\n",
      "0        3  0.7327  0.8020  0.7030\n",
      "1        5  0.7327  0.8020  0.7030\n",
      "2        7  0.7228  0.8020  0.7030\n",
      "('Archivo: ', 'data/Iris.csv')\n",
      "  C|kernel  linear    poly     rbf\n",
      "0        3  0.3400  0.3600  0.3400\n",
      "1        5  0.3400  0.3400  0.3400\n",
      "2        7  0.3400  0.3400  0.3400\n"
     ]
    }
   ],
   "source": [
    "#EXPERIMENTO II\n",
    "#iris>0.9\n",
    "\n",
    "def experimentII():\n",
    "    files = [\"data/heart.csv\", \"data/Iris.csv\"]\n",
    "    kind_kernel = [\"linear\", \"poly\", \"rbf\"]\n",
    "    setC = [3,5,7]\n",
    "    k = 3\n",
    "    gamma = 1\n",
    "    for it_file in files:\n",
    "        print(\"Archivo: \", it_file)\n",
    "        data = read_file(it_file)\n",
    "        X, y = X_y(data.values)\n",
    "        data_X = normalize_data(X)\n",
    "        data = np.concatenate((data_X, y), axis=1)\n",
    "        kfolds, sz_fold = create_kfolds(data, k)\n",
    "        y_val = np.unique(y) # values y\n",
    "        \n",
    "        result_tb = [setC]\n",
    "        \n",
    "        for kind in kind_kernel:\n",
    "            _setC = []\n",
    "            for C in setC:\n",
    "                accuracy = 0.0\n",
    "                \n",
    "                for l in range(y_val.shape[0]):\n",
    "                    for i in range(k):\n",
    "                        X_train = np.zeros((sz_fold * (k-1), data.shape[1] - 1))\n",
    "                        y_train = np.zeros((sz_fold * (k-1), 1))\n",
    "                        X_test = np.zeros((sz_fold, data.shape[1] - 1))\n",
    "                        y_test = np.zeros((sz_fold, 1))\n",
    "                        count_sz_fold = 0\n",
    "                        for j in range(k):\n",
    "                            if j == i:\n",
    "                                X_test = kfolds[i]['X']\n",
    "                                y_test = kfolds[i]['y']\n",
    "                            else:\n",
    "                                X_train[count_sz_fold:count_sz_fold+sz_fold, :] = kfolds[j]['X']\n",
    "                                y_train[count_sz_fold:count_sz_fold+sz_fold, :] = kfolds[j]['y'] == y_val[l]\n",
    "                                count_sz_fold += sz_fold\n",
    "\n",
    "                        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                        lab_enc = preprocessing.LabelEncoder()\n",
    "                        training_scores_encoded = lab_enc.fit_transform(y_train)\n",
    "                        \n",
    "                        tlab_enc = preprocessing.LabelEncoder()\n",
    "                        testing_scores_encoded = tlab_enc.fit_transform(y_test)\n",
    "                        \n",
    "                        if kind == 'linear':\n",
    "                            #LINEAL\n",
    "                            svm_reg = SVC(kernel= kind, C=C)\n",
    "                            svm_reg.fit(X_train, training_scores_encoded)\n",
    "                            accuracy_linear = svm_reg.score(X_test, testing_scores_encoded)\n",
    "                            accuracy = accuracy_linear\n",
    "                            #print('accuracy linear: ',accuracy_linear)\n",
    "                        elif kind == 'poly':\n",
    "                            #POLINOMIAL\n",
    "                            svm_reg = SVC(kernel= kind, C=C, degree=3, gamma=gamma)\n",
    "                            svm_reg.fit(X_train, training_scores_encoded)\n",
    "                            accuracy_poly = svm_reg.score(X_test, testing_scores_encoded)\n",
    "                            accuracy = accuracy_poly\n",
    "                            #print('accuracy polinomial: ',accuracy_poly)\n",
    "                        else:\n",
    "                            #GAUSSIANO\n",
    "                            svm_reg = SVC(kernel= kind, C=C, gamma=gamma)\n",
    "                            svm_reg.fit(X_train, training_scores_encoded)\n",
    "                            accuracy_gauss = svm_reg.score(X_test, testing_scores_encoded)\n",
    "                            accuracy = accuracy_gauss \n",
    "                            #print('accuracy gaussiano: ',accuracy_gauss)\n",
    "\n",
    "                _setC.append(\"%.4f\" % accuracy)\n",
    "            result_tb.append(_setC)\n",
    "\n",
    "        m = np.asarray(result_tb)\n",
    "        pdObj = pd.DataFrame(m.T[:], columns=['C|kernel',\"linear\", \"poly\", \"rbf\"]) \n",
    "        print(pdObj)          \n",
    "experimentII()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
