{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leer_Datos(filename):\n",
    "    pd = pd.read_csv(filename, delim_whitespace=True)\n",
    "    return np.array(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = np.mean(data) \n",
    "    std_ = np.std(data) #data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(theta,X):\n",
    "    return 1 / (1 + np.exp( -np.dot(X,theta) ) )        \n",
    "    #return 1 / (1 + np.exp( -np.dot(X,theta.T) ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data): \n",
    "    porcentage = 0.70\n",
    "    np.random.shuffle(data)\n",
    "    rows = int(porcentage * len(data))\n",
    "    #int((70*len(data))/100)\n",
    "    train = data[:rows, :]\n",
    "    test = data[rows:, :]\n",
    "    #X_train = data[:rows, :col]\n",
    "    #y_train = data[:rows, col]\n",
    "    #X_test = data[rows:, :col]\n",
    "    #y_test = data[rows:, col]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_funcion_costo(X,y, theta):\n",
    "    m = y.shape[0]\n",
    "    predictions = Sigmoidal(theta,X)\n",
    "    error = (y * np.log(predictions)) - ((1-y) * np.log(1-predictions))\n",
    "    #error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\n",
    "    return -1/m * (np.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "    for i in range(nro_iter):\n",
    "        pred = Sigmoidal(theta, X)\n",
    "        pred = pred - y   \n",
    "        cost_history[i] = calcular_funcion_costo( X, y, theta) \n",
    "        theta = theta - (learning_rate * ((np.matmul(X.T,pred))/m))\n",
    "        #theta = theta - (learning_rate * ((np.matmul(pred,X))/m))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    name_col = data.columns[data.shape[1]-1]\n",
    "    cant_x_class = data[name_col].value_counts()\n",
    "    \n",
    "    data = np.array(data) \n",
    "    #X, y = divide_X_y(data)\n",
    "    i = 0   \n",
    "    k_fold = []\n",
    "    for c in cant_x_class:\n",
    "        while ( c%k != 0 ):\n",
    "            c=c-1\n",
    "        #k_data_X.append(np.split(X[i:c+i, :], k))\n",
    "        #k_data_y.append(np.split(y[i:c+i, :], k))\n",
    "        \n",
    "        k_data = np.split(data[i:c+i, :], k)\n",
    "        X, y = divide_X_y(data)\n",
    "        k_fold.append({\"X\": X, \"y\" : y})\n",
    "        i = c+i\n",
    "    #return k_data_X, k_data_y    \n",
    "    return k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predict = Sigmoidal(theta, X)\n",
    "    #print(\"predict: \",predict)\n",
    "    probab_threshold = 0.5  \n",
    "    predicted_classes = (predict >= probab_threshold)\n",
    "    result = np.logical_xor(np.logical_not(predicted_classes), y)\n",
    "    return np.sum(result) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_X_y(data):\n",
    "    col = data.shape[1]-1\n",
    "    X = data[:, :col]\n",
    "    y = data[:, col:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica\n",
      "[1.0, 1.0, 0.9523809523809523]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1():\n",
    "    #data = Leer_Datos('Iris.csv')\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cant_x_class = data[data.columns[data.shape[1]-1]].value_counts()\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = []    \n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    data = np.array(data)     \n",
    "    X, y = divide_X_y(data)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada = np.concatenate((X_normalizada, y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)   \n",
    "    \n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        for j in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[i], categories_train[j]), axis=0)\n",
    "            #tmp_data = np.concatenate([np.expand_dims(i,axis=0) for i in [y_classes[i],y_classes[j]]])\n",
    "            \n",
    "            posiciones_a_cambiar = np.where(tmp_data == y_classes[i]) \n",
    "            tmp_data = np.c_[tmp_data[:, :tmp_data.shape[1]-1], np.zeros(tmp_data.shape[0])]\n",
    "            tmp_data[posiciones_a_cambiar] = 1\n",
    "            np.random.shuffle(tmp_data)\n",
    "            \n",
    "            set_train, set_test = train_test(tmp_data)\n",
    "            X_train, y_train = divide_X_y(set_train)\n",
    "            X_test, y_test = divide_X_y(set_test)\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "            theta = np.random.rand(X_train.shape[1])\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "            \n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "            \n",
    "    #print(W_array)\n",
    "    print('Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = [] \n",
    "    accuracy_array_test = []\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)  \n",
    "        \n",
    "    # k_set_X[0], k_set_y[0] return first element of e/array\n",
    "    #k_set_X, k_set_y = create_k_folds(pd.DataFrame(data_normalizada), k)    \n",
    "    #k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        for c1 in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[c], categories_train[c1]), axis=0)\n",
    "            k_set = create_k_folds(pd.DataFrame(tmp_data), k)\n",
    "            print('...')\n",
    "            total_accuracy_test = 0\n",
    "            for i_test in range(0, k): \n",
    "                X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "                y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "                X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "                y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "                print(',,,,,,,,,,')\n",
    "                print(X_train)\n",
    "                print(y_train)\n",
    "                for j in range(0, k): \n",
    "                    if (i_test == j):\n",
    "                        X_test = k_set[i_test]['X']\n",
    "                        y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                        #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                    else:\n",
    "                        X_train = k_set[j]['X']\n",
    "                        y_train = k_set[j]['y'] == y_classes[c]  \n",
    "                print(',,,,,,,,,,')\n",
    "                print(X_train)\n",
    "                print(y_train)\n",
    "                y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                theta = np.random.rand(np.size(X_train[0]))\n",
    "                X_train = X_train.astype(float)\n",
    "                y_train = y_train.astype(int)\n",
    "                W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "                W_array.append(W)\n",
    "\n",
    "                X_test = X_test.astype('float')\n",
    "                y_test = y_test.astype(int)\n",
    "                accuracy_test = accuracy(X_test, y_test, W)\n",
    "                total_accuracy_test += accuracy_test\n",
    "\n",
    "            total_accuracy_test = total_accuracy_test / k\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.78125, 0.96875]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) \n",
    "\n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "\n",
    "    for c in range(y_classes.shape[0]):  \n",
    "        tmp_data = np.c_[train[:, :train.shape[1]-1], np.zeros(train.shape[0])]\n",
    "        tmp_data[np.where(train == y_classes[c])] = 1\n",
    "        \n",
    "        set_train, set_test = train_test(tmp_data)\n",
    "        X_train, y_train = divide_X_y(set_train)\n",
    "        X_test, y_test = divide_X_y(set_test)\n",
    "        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "        theta = np.random.rand(X_train.shape[1])\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "        W_array.append(W)\n",
    "            \n",
    "        X_test = X_test.astype('float')\n",
    "        y_test = y_test.astype(int)\n",
    "        accuracy_test = accuracy(X_test, y_test, W)\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "        \n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.7, 0.9533333333333334]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "        \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "   \n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        total_accuracy_test = 0\n",
    "        for i_test in range(0, k): \n",
    "            X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "            y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "            X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "            y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "     \n",
    "            for j in range(0, k): \n",
    "                if (i_test == j):\n",
    "                    X_test = k_set[i_test]['X']\n",
    "                    y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                    #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                else:\n",
    "                    X_train = k_set[j]['X']\n",
    "                    y_train = k_set[j]['y'] == y_classes[c]                   \n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "            \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "          \n",
    "            theta = np.random.rand(np.size(X_train[0]))\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "\n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            total_accuracy_test += accuracy_test\n",
    "            \n",
    "        total_accuracy_test = total_accuracy_test / k\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_find_parameters():\n",
    "    fdata = load_data('petrol_consumption.csv')\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = train_test(data,data.shape[1]-1)\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "    #theta = np.random.rand(n_features)\n",
    "\n",
    "    epochs = [2,10,50,100] \n",
    "    learn_rates = [ 0.1,  0.5 , 1]\n",
    "    \n",
    "    result_train = np.empty([len(learn_rates),len(epochs)])\n",
    "    result_test = np.empty([len(learn_rates),len(epochs)])\n",
    "    for epoch in epochs:\n",
    "        for learn_rate in learn_rates:\n",
    "            theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "            error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "            error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "\n",
    "            print(\"result_train e \", epoch , \"lr \", learn_rate,\":\", error_train)\n",
    "            \n",
    "            result_train = error_train\n",
    "            result_test = error_test\n",
    "        print(\"\\n\")\n",
    "    #pdObj = pd.DataFrame(result_train, index = learn_rates, columns = epochs) \n",
    "    #pdObj1 = pd.DataFrame(result_test, index = learn_rates, columns = epochs) \n",
    "    #return pdObj\n",
    "GD_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_ploteo():\n",
    "    fdata = load_data('petrol_consumption.csv')\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "    X_train, y_train, X_test, y_test = train_test(data,data.shape[1]-1)\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    #theta = np.zeros(n_features)\n",
    "    theta = np.random.rand(n_features)\n",
    "\n",
    "    epochs = [1500,1800,11100,114000,11700,12000,12300,12600,12900,13200]  \n",
    "    learn_rate = 0.0005\n",
    "\n",
    "    for epoch in epochs:\n",
    "        theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "        theta_gd1, cost_history1 = gradient_descent(X_test, y_test, theta, epoch, learn_rate)\n",
    "        error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "        error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(range(len(cost_history)), cost_history)\n",
    "        plt.plot(range(len(cost_history1)), cost_history1)\n",
    "        plt.title('TRAIN - TEST '+str(epoch), {'fontsize':10})\n",
    "        print(\"Weights of gradient_descent - training data: \", theta_gd, \"\\n\")\n",
    "        print(\"Weights of gradient_descent - testing data: \", theta_gd1, \"\\n\")\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "GD_ploteo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1\n",
    "    num_iter = 10000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    k_set_X, k_set_y = crear_k_folds(data_normalizada, k)\n",
    "    #k_folds, size_fold = crear_k_folds(data_normalizada, k)\n",
    "\n",
    "    W_vec = []\n",
    "    y_values = np.unique(y)\n",
    "    acc_test_total_vec = []\n",
    "\n",
    "    for l in range(y_values.shape[0]):\n",
    "        acc_test_total = 0.0\n",
    "        for i in range(k):\n",
    "            X_train = np.zeros((size_fold * (k-1), norm_data.shape[1] - 1))\n",
    "            X_test = np.zeros((size_fold, norm_data.shape[1] - 1))\n",
    "            y_train = np.zeros((size_fold * (k-1), 1))\n",
    "            y_test = np.zeros((size_fold, 1))\n",
    "\n",
    "            count_sz_fold = 0\n",
    "            for j in range(k):\n",
    "                if j == i:\n",
    "                    X_test = k_folds[i]['X']\n",
    "                    y_test = k_folds[i]['y'] == y_values[l]\n",
    "                else:\n",
    "                    X_train[count_sz_fold:count_sz_fold+size_fold, :] = k_folds[j]['X']\n",
    "                    y_train[count_sz_fold:count_sz_fold+size_fold, :] = k_folds[j]['y'] == y_values[l]\n",
    "                    count_sz_fold += size_fold\n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "            W = Crear_Pesos(X_train)\n",
    "            W, costs = Gradiente_Descendiente(X_train, y_train, W, num_iter, learn_rate)\n",
    "            X_test = X_test.astype('float')\n",
    "            acc_test = Calcular_Accuraccy(X_test, y_test, W)\n",
    "            acc_test_total += acc_test\n",
    "\n",
    "        acc_test_total /= k\n",
    "        acc_test_total_vec.append(acc_test_total)\n",
    "\n",
    "    print(\"Clasificación multiclase 'uno vs todos'\")\n",
    "    print(\"Accuracy en los datos de prueba para 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' respectivamente:\")\n",
    "    print(acc_test_total_vec)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-155-7acb32f81086>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-155-7acb32f81086>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    category_2 = train[train[:, train.shape[1]-1] == y_classes[1]]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " category_1 = train[train[:, train.shape[1]-1] == y_classes[0]]\n",
    "    category_2 = train[train[:, train.shape[1]-1] == y_classes[1]]\n",
    "    category_3 = train[train[:, train.shape[1]-1] == y_classes[2]]\n",
    "    s = np.array([category_1, category_2, category_3])\n",
    "    \n",
    "    print('s: ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
