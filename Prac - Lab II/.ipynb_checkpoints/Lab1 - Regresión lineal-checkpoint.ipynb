{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    mean_ = data.mean(axis=0) \n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data,col):\n",
    "    X_train = data[:int((70*len(data))/100), :col]\n",
    "    y_train = data[:int((70*len(data))/100), col]\n",
    "    X_test = data[int((70*len(data))/100):, :col]\n",
    "    y_test = data[int((70*len(data))/100):, col]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(theta, X):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(X, y, theta):\n",
    "    #J(theta) = 1/2m sum(h(x) - y)^2\n",
    "    m = X.shape[0] #nbr of training data \n",
    "    pred = prediction(theta, X)\n",
    "    c = (1/(2*m)) * np.sum(np.square(pred - y))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "    #h = calculate_cost(X, y, theta)\n",
    "    for i in range(nro_iter):\n",
    "        pred = prediction(theta, X)\n",
    "        pred = pred - y        \n",
    "        theta = theta - (learning_rate * (np.dot(X.T,pred)/m))\n",
    "        cost_history[i] = calculate_cost( X, y, theta) \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    theta = np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y))\n",
    "    #theta = np.linalg.inv(X.T.dot(X)).dot((X.T).dot(y))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'petrol_consumption.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1f612bb5a27e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cost: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mNE_find_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-1f612bb5a27e>\u001b[0m in \u001b[0;36mNE_find_parameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNE_find_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'petrol_consumption.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-72522cc71622>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'petrol_consumption.csv'"
     ]
    }
   ],
   "source": [
    "def NE_find_parameters():\n",
    "    fdata = load_data('petrol_consumption.csv')\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "    X_train, y_train, X_test, y_test = train_test(data,data.shape[1]-1)\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    theta = normal_equation(X_train, y_train)\n",
    "    print(\"Weight Normal Equation Train: \", theta)\n",
    "    cost = calculate_cost(X_train, y_train, theta)\n",
    "    print(\"Cost: \", cost)\n",
    "\n",
    "    theta = normal_equation(X_test, y_test)\n",
    "    print(\"Weight Normal Equation Test: \", theta)\n",
    "    cost = calculate_cost(X_test, y_test, theta)\n",
    "    print(\"Cost: \", cost)\n",
    "\n",
    "NE_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_find_parameters():\n",
    "    fdata = load_data('petrol_consumption.csv')\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = train_test(data,data.shape[1]-1)\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "    #theta = np.random.rand(n_features)\n",
    "\n",
    "    epochs = [2,10,50,100] \n",
    "    learn_rates = [ 0.1,  0.5 , 1]\n",
    "    \n",
    "    result_train = np.empty([len(learn_rates),len(epochs)])\n",
    "    result_test = np.empty([len(learn_rates),len(epochs)])\n",
    "    for epoch in epochs:\n",
    "        for learn_rate in learn_rates:\n",
    "            theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "            error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "            error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "\n",
    "            print(\"result_train e \", epoch , \"lr \", learn_rate,\":\", error_train)\n",
    "            \n",
    "            result_train = error_train\n",
    "            result_test = error_test\n",
    "        print(\"\\n\")\n",
    "    #pdObj = pd.DataFrame(result_train, index = learn_rates, columns = epochs) \n",
    "    #pdObj1 = pd.DataFrame(result_test, index = learn_rates, columns = epochs) \n",
    "    #return pdObj\n",
    "GD_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_ploteo():\n",
    "    fdata = load_data('petrol_consumption.csv')\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "    X_train, y_train, X_test, y_test = train_test(data,data.shape[1]-1)\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    #theta = np.zeros(n_features)\n",
    "    theta = np.random.rand(n_features)\n",
    "\n",
    "    epochs = [1500,1800,11100,114000,11700,12000,12300,12600,12900,13200]  \n",
    "    learn_rate = 0.0005\n",
    "\n",
    "    for epoch in epochs:\n",
    "        theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "        theta_gd1, cost_history1 = gradient_descent(X_test, y_test, theta, epoch, learn_rate)\n",
    "        error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "        error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(range(len(cost_history)), cost_history)\n",
    "        plt.plot(range(len(cost_history1)), cost_history1)\n",
    "        plt.title('TRAIN - TEST '+str(epoch), {'fontsize':10})\n",
    "        print(\"Weights of gradient_descent - training data: \", theta_gd, \"\\n\")\n",
    "        print(\"Weights of gradient_descent - testing data: \", theta_gd1, \"\\n\")\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "GD_ploteo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HNE_find_parameters():\n",
    "    fdata = pd.read_csv('Housing.csv')\n",
    "    fdata = fdata.replace(to_replace='yes',value=1,regex=True)\n",
    "    fdata = fdata.replace(to_replace='no',value=0,regex=True)\n",
    "\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "\n",
    "    X_train = data[:int((70*len(data))/100), 1:]\n",
    "    y_train = data[:int((70*len(data))/100), 0]\n",
    "    X_test = data[int((70*len(data))/100):, 1:]\n",
    "    y_test = data[int((70*len(data))/100):, 0]\n",
    "\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    theta = normal_equation(X_train, y_train)\n",
    "    print(\"Weight Normal Equation Train: \", theta)\n",
    "    cost = calculate_cost(X_train, y_train, theta)\n",
    "    print(\"Cost: \", cost)\n",
    "\n",
    "    theta = normal_equation(X_test, y_test)\n",
    "    print(\"Weight Normal Equation Test: \", theta)\n",
    "    cost = calculate_cost(X_test, y_test, theta)\n",
    "    print(\"Cost: \", cost)\n",
    "\n",
    "HNE_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HGD_find_parameters():\n",
    "    fdata = pd.read_csv('Housing.csv')\n",
    "    fdata = fdata.replace(to_replace='yes',value=1,regex=True)\n",
    "    fdata = fdata.replace(to_replace='no',value=0,regex=True)\n",
    "\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "\n",
    "    X_train = data[:int((70*len(data))/100), 1:]\n",
    "    y_train = data[:int((70*len(data))/100), 0]\n",
    "    X_test = data[int((70*len(data))/100):, 1:]\n",
    "    y_test = data[int((70*len(data))/100):, 0]\n",
    "\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "\n",
    "    epochs = [2,10,50,100] \n",
    "    learn_rates = [0.0005, 0.1,  0.5 , 1]\n",
    "    \n",
    "    result_train = np.empty([len(learn_rates),len(epochs)])\n",
    "    result_test = np.empty([len(learn_rates),len(epochs)])\n",
    "    for epoch in epochs:\n",
    "        for learn_rate in learn_rates:\n",
    "            theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "            error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "            error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "            \n",
    "            print(\"result_train e \", epoch , \"lr \", learn_rate,\":\", error_train)\n",
    "            #print (\"result_test e \", epoch , \"lr \", learn_rate,\":\", error_test)\n",
    "\n",
    "        print(\"\\n\")\n",
    "            \n",
    "        #pdObj = pd.DataFrame(result_train, index = learn_rates, columns = epochs) \n",
    "        #pdObj1 = pd.DataFrame(result_test, index = learn_rates, columns = epochs) \n",
    "    #return pdObj\n",
    "\n",
    "HGD_find_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HGD_ploteo():\n",
    "    fdata = pd.read_csv('Housing.csv')\n",
    "    fdata = fdata.replace(to_replace='yes',value=1,regex=True)\n",
    "    fdata = fdata.replace(to_replace='no',value=0,regex=True)\n",
    "\n",
    "    data = normalization(fdata)\n",
    "    data = data.values\n",
    "\n",
    "    X_train = data[:int((70*len(data))/100), 1:]\n",
    "    y_train = data[:int((70*len(data))/100), 0]\n",
    "    X_test = data[int((70*len(data))/100):, 1:]\n",
    "    y_test = data[int((70*len(data))/100):, 0]\n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "\n",
    "    epochs = [1500,1800,11100,114000,11700,12000,12300,12600,12900,13200]  \n",
    "    learn_rate = 0.0005\n",
    "\n",
    "    for epoch in epochs:\n",
    "        theta_gd, cost_history = gradient_descent(X_train, y_train, theta, epoch, learn_rate)\n",
    "        theta_gd1, cost_history1 = gradient_descent(X_test, y_test, theta, epoch, learn_rate)\n",
    "        error_train = calculate_cost(X_train, y_train, theta_gd)\n",
    "        error_test = calculate_cost(X_test, y_test, theta_gd)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(range(len(cost_history)), cost_history)\n",
    "        plt.plot(range(len(cost_history1)), cost_history1)\n",
    "        plt.title('TRAIN - TEST '+str(epoch), {'fontsize':10})\n",
    "        print(\"Weights of gradient_descent - training data: \", theta_gd, \"\\n\")\n",
    "        print(\"Weights of gradient_descent - testing data: \", theta_gd1, \"\\n\")\n",
    "        ax.grid(True)\n",
    "        plt.show()\n",
    "HGD_ploteo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
