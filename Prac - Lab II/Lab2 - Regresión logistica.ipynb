{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leer_Datos(filename,atrributes):\n",
    "    data = pd.read_csv(filename, usecols=atrributes) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = data.mean(axis=0)\n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(theta,X):\n",
    "    return 1 / (1 + np.exp( -np.dot(X,theta) ) )        \n",
    "    #return 1 / (1 + np.exp( -np.dot(X,theta.T) ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data): \n",
    "    porcentage = 0.70\n",
    "    np.random.shuffle(data)\n",
    "    rows = int(porcentage * len(data))\n",
    "    #int((70*len(data))/100)\n",
    "    train = data[:rows, :]\n",
    "    test = data[rows:, :]\n",
    "    #X_train = data[:rows, :col]\n",
    "    #y_train = data[:rows, col]\n",
    "    #X_test = data[rows:, :col]\n",
    "    #y_test = data[rows:, col]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_X_y(data):\n",
    "    col = data.shape[1]-1\n",
    "    X = data[:, :col]\n",
    "    y = data[:, col:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_funcion_costo(X,y, theta):\n",
    "    m = y.shape[0]\n",
    "    predictions = Sigmoidal(theta,X)\n",
    "    error = (y * np.log(predictions)) - ((1-y) * np.log(1-predictions))\n",
    "    #error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\n",
    "    return -1/m * (np.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "    for i in range(nro_iter):\n",
    "        pred = Sigmoidal(theta, X)\n",
    "        pred = pred - y   \n",
    "        cost_history[i] = calcular_funcion_costo( X, y, theta) \n",
    "        #print((learning_rate * ((np.matmul(X.T,pred))/m)).shape)\n",
    "        #print(theta.shape)\n",
    "        theta = theta - (learning_rate * ((np.matmul(X.T,pred))/m))\n",
    "        #theta = theta - (learning_rate * ((np.matmul(pred,X))/m))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predict = Sigmoidal(theta, X)\n",
    "    #print(\"predict: \",predict)\n",
    "    probab_threshold = 0.5  \n",
    "    predicted_classes = (predict >= probab_threshold)\n",
    "    result = np.logical_xor(np.logical_not(predicted_classes), y)\n",
    "    return np.sum(result) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    col_class = data[:,-1]\n",
    "    #col_class = np.array([0,0,1,0,1,0,1,0,0,0])\n",
    "    num_rows = col_class.shape[0]\n",
    "    unique, counts = np.unique(col_class, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    print(distribution)\n",
    "    percent_of_first_class = round((distribution[0.0] * 100) /  num_rows)\n",
    "   \n",
    "    percent_of_second_class = round((distribution[1.0] * 100) / num_rows)\n",
    "   \n",
    "    num_per_fold = round(num_rows/k)\n",
    "    \n",
    "    num_first_class_per_fold = round((num_per_fold * percent_of_first_class)/100)\n",
    "    num_second_class_per_fold = round((num_per_fold * percent_of_second_class)/100)\n",
    "\n",
    "    list_indices=[]\n",
    "    num_0s=[]\n",
    "    num_1s=[]\n",
    "    \n",
    "    for i in range(k):\n",
    "        list_indices.append([])\n",
    "        num_0s.append(0)\n",
    "        num_1s.append(0)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "    \n",
    "        if col_class[i] == 0.0:\n",
    "            for j in range(k):\n",
    "                if(num_0s[j]<num_first_class_per_fold):\n",
    "                    num_0s[j]+=1\n",
    "                    #print(list_indices[j],list_indices[j].count(0))\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for j in range(k):\n",
    "                if(num_1s[j]<num_second_class_per_fold):\n",
    "                    num_1s[j]+=1\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "    if num_rows%k!=0:\n",
    "        list_indices[k-1].append(col_class.shape[0]-1)\n",
    "    \n",
    "    return list_indices\n",
    "    \n",
    "    #print(list_indices[0])\n",
    "    #print(list_indices[1])\n",
    "    #print(list_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareXandY(training,test):\n",
    "    train_X = training[:,:-1]\n",
    "    train_X = np.concatenate((np.ones([train_X.shape[0], 1]), train_X), axis=1)\n",
    "\n",
    "    test_X = test[:,:-1]\n",
    "    test_X = np.concatenate((np.ones([test_X.shape[0], 1]), test_X), axis=1)\n",
    "\n",
    "    train_y = training[:,-1]\n",
    "    test_y = test[:,-1]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tasa:  0.01  iteracion:  500\n",
      "0.7796300991345294\n",
      "tasa:  0.01  iteracion:  1000\n",
      "0.7796300991345294\n",
      "tasa:  0.01  iteracion:  1500\n",
      "0.7796444119792368\n",
      "tasa:  0.01  iteracion:  2000\n",
      "0.7813175470662505\n",
      "tasa:  0.01  iteracion:  2500\n",
      "0.7871301875517639\n",
      "tasa:  0.01  iteracion:  3000\n",
      "0.7924782512616516\n",
      "tasa:  0.01  iteracion:  3500\n",
      "0.7961536810653221\n",
      "tasa:  0.05  iteracion:  500\n",
      "0.7871587774425506\n",
      "tasa:  0.05  iteracion:  1000\n",
      "0.800736097079931\n",
      "tasa:  0.05  iteracion:  1500\n",
      "0.8011875781642116\n",
      "tasa:  0.05  iteracion:  2000\n",
      "0.8015451844901285\n",
      "tasa:  0.05  iteracion:  2500\n",
      "0.8015382249602304\n",
      "tasa:  0.05  iteracion:  3000\n",
      "0.8015455066777832\n",
      "tasa:  0.05  iteracion:  3500\n",
      "0.8016171066999488\n",
      "tasa:  0.1  iteracion:  500\n",
      "0.8007217842352233\n",
      "tasa:  0.1  iteracion:  1000\n",
      "0.8015451844901285\n",
      "tasa:  0.1  iteracion:  1500\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7e42855b3fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mGD_find_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-7e42855b3fd7>\u001b[0m in \u001b[0;36mGD_find_parameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mtheta_test3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mtheta_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mtheta_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mtheta_test3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-038c125271db>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, theta, nro_iter, learning_rate)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSigmoidal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcost_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcular_funcion_costo\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m#print((learning_rate * ((np.matmul(X.T,pred))/m)).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print(theta.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-44d5be37c6d7>\u001b[0m in \u001b[0;36mcalcular_funcion_costo\u001b[1;34m(X, y, theta)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSigmoidal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m#error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def GD_find_parameters():\n",
    "    files = [(\"Data_classification/weatherAUS.csv\",[\"MinTemp\",\"MaxTemp\",\"RainToday\",\"RainTomorrow\"])]\n",
    "    for f in files:\n",
    "        f_name = f[0]\n",
    "        atrributes = f[1]\n",
    "        k = 3\n",
    "        data = Leer_Datos(f_name,atrributes)\n",
    "        data = data.replace(to_replace='Yes',value=1,regex=True)\n",
    "        data = data.replace(to_replace='No',value=0,regex=True)\n",
    "\n",
    "        data = data.values[~np.isnan(data).any(axis=1)]\n",
    "        data[:,0:2] = Normalizar_Datos(data[:,0:2])\n",
    "      \n",
    "        indices = create_k_folds(data, k)\n",
    "        #print(indices)\n",
    "\n",
    "        fold1 = data[indices[0]]\n",
    "        fold2 = data[indices[1]]\n",
    "        fold3 = data[indices[2]]\n",
    "\n",
    "\n",
    "        learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        \n",
    "        training1 = np.concatenate((fold1,fold2))\n",
    "        training2 = np.concatenate((fold3,fold2))\n",
    "        training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "        train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "        train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "        train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "\n",
    "        for tasa in learning_rate:\n",
    "            for ite in range(500,3501,500):\n",
    "                print(\"tasa: \", tasa, \" iteracion: \", ite)\n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                theta_test1, cost_history1 = gradient_descent(train_X1, train_y1, theta_test1, ite, tasa)\n",
    "                theta_test2, cost_history2 = gradient_descent(train_X2, train_y2, theta_test2, ite, tasa) \n",
    "                theta_test3, cost_history3 = gradient_descent(train_X3, train_y3, theta_test3, ite, tasa) \n",
    "                \n",
    "                \n",
    "                accuracy1 = accuracy(test_X1 , test_y1, theta_test1)\n",
    "                accuracy2 = accuracy(test_X2 , test_y2, theta_test2)\n",
    "                accuracy3 = accuracy(test_X3 , test_y3, theta_test3)\n",
    "\n",
    "                print((accuracy1+accuracy2+accuracy3)/3)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "GD_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tasa:  0.01  iteracion:  500\n",
      "0.6298872657811344\n",
      "tasa:  0.01  iteracion:  1000\n",
      "0.5973229763600428\n",
      "tasa:  0.01  iteracion:  1500\n",
      "0.6299397131405479\n",
      "tasa:  0.01  iteracion:  2000\n",
      "0.8053650888292645\n",
      "tasa:  0.01  iteracion:  2500\n",
      "0.7766101339339936\n",
      "tasa:  0.01  iteracion:  3000\n",
      "0.811112215266046\n",
      "tasa:  0.01  iteracion:  3500\n",
      "0.7507701480671768\n",
      "tasa:  0.05  iteracion:  500\n",
      "0.5751046186800931\n",
      "tasa:  0.05  iteracion:  1000\n",
      "0.5607864895602152\n",
      "tasa:  0.05  iteracion:  1500\n",
      "0.7419700332350635\n",
      "tasa:  0.05  iteracion:  2000\n",
      "0.6415305795709254\n",
      "tasa:  0.05  iteracion:  2500\n",
      "0.6270385460487815\n",
      "tasa:  0.05  iteracion:  3000\n",
      "0.7515761811697418\n",
      "tasa:  0.05  iteracion:  3500\n",
      "0.6798585577528238\n",
      "tasa:  0.1  iteracion:  500\n",
      "0.5408068060110195\n",
      "tasa:  0.1  iteracion:  1000\n",
      "0.6147051354246028\n",
      "tasa:  0.1  iteracion:  1500\n",
      "0.6279273907714731\n",
      "tasa:  0.1  iteracion:  2000\n",
      "0.7506045248269237\n",
      "tasa:  0.1  iteracion:  2500\n",
      "0.6472832267823821\n",
      "tasa:  0.1  iteracion:  3000\n",
      "0.6385355593096823\n",
      "tasa:  0.1  iteracion:  3500\n",
      "0.7611657667803945\n",
      "tasa:  0.2  iteracion:  500\n",
      "0.5792424392990824\n",
      "tasa:  0.2  iteracion:  1000\n",
      "0.7066591584131086\n",
      "tasa:  0.2  iteracion:  1500\n",
      "0.530237282895536\n",
      "tasa:  0.2  iteracion:  2000\n",
      "0.6069263639073834\n",
      "tasa:  0.2  iteracion:  2500\n",
      "0.7354058321463667\n",
      "tasa:  0.2  iteracion:  3000\n",
      "0.6854731855974031\n",
      "tasa:  0.2  iteracion:  3500\n",
      "0.6376025483895901\n",
      "tasa:  0.3  iteracion:  500\n",
      "0.6826989963231641\n",
      "tasa:  0.3  iteracion:  1000\n",
      "0.6383726964567669\n",
      "tasa:  0.3  iteracion:  1500\n",
      "0.5599749356829751\n",
      "tasa:  0.3  iteracion:  2000\n",
      "0.7363581657778219\n",
      "tasa:  0.3  iteracion:  2500\n",
      "0.6701281923879558\n",
      "tasa:  0.3  iteracion:  3000\n",
      "0.6270523479854693\n",
      "tasa:  0.3  iteracion:  3500\n",
      "0.7536133470248546\n",
      "tasa:  0.4  iteracion:  500\n",
      "0.5848681086930118\n",
      "tasa:  0.4  iteracion:  1000\n",
      "0.7141894950699482\n",
      "tasa:  0.4  iteracion:  1500\n",
      "0.706661918800446\n",
      "tasa:  0.4  iteracion:  2000\n",
      "0.5963540804045624\n",
      "tasa:  0.4  iteracion:  2500\n",
      "0.696015104839511\n",
      "tasa:  0.4  iteracion:  3000\n",
      "0.7277485176719997\n",
      "tasa:  0.4  iteracion:  3500\n",
      "0.6538805525191295\n"
     ]
    }
   ],
   "source": [
    "def GD_find_parameters1():\n",
    "    files = [ (\"Data_classification/titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"Data_classification/gender_submission.csv\",[\"Survived\"]),\n",
    "              (\"Data_classification/titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]) ]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "    k = 3\n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='female',value=1,regex=True)\n",
    "    data = data.replace(to_replace='male',value=0,regex=True)\n",
    "    data = data.replace(to_replace='C',value=0,regex=True)\n",
    "    data = data.replace(to_replace='S',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    f_name = files[1][0]\n",
    "    atrributes = files[1][1]\n",
    "    data2 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = np.concatenate((data, data2), axis=1)\n",
    "\n",
    "    f_name = files[2][0]\n",
    "    atrributes = files[2][1]\n",
    "    data3 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data3 = data3[[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]]\n",
    "\n",
    "    data3 = data3.replace(to_replace='female',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='male',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='C',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='S',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "\n",
    "    \n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data[:,1:2] = Normalizar_Datos(data[:,1:2])\n",
    "      \n",
    "    indices = create_k_folds(data, k)\n",
    "        #print(indices)\n",
    "\n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "\n",
    "\n",
    "    learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "\n",
    "    for tasa in learning_rate:\n",
    "        for ite in range(500,3501,500):\n",
    "            print(\"tasa: \", tasa, \" iteracion: \", ite)\n",
    "            theta_test1 = np.zeros(train_X1.shape[1])\n",
    "            theta_test2 = np.zeros(train_X2.shape[1])\n",
    "            theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "            theta_test1, cost_history1 = gradient_descent(train_X1, train_y1, theta_test1, ite, tasa)\n",
    "            theta_test2, cost_history2 = gradient_descent(train_X2, train_y2, theta_test2, ite, tasa) \n",
    "            theta_test3, cost_history3 = gradient_descent(train_X3, train_y3, theta_test3, ite, tasa) \n",
    "                \n",
    "                \n",
    "            accuracy1 = accuracy(test_X1 , test_y1, theta_test1)\n",
    "            accuracy2 = accuracy(test_X2 , test_y2, theta_test2)\n",
    "            accuracy3 = accuracy(test_X3 , test_y3, theta_test3)\n",
    "\n",
    "            print((accuracy1+accuracy2+accuracy3)/3)\n",
    " \n",
    "\n",
    "        \n",
    "GD_find_parameters1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_ploteo():\n",
    "    files = [ (\"Data_classification/titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"Data_classification/titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]),\n",
    "              (\"Data_classification/gender_submission.csv\",[\"Survived\"]) ]\n",
    "    \n",
    "    Xdata = []\n",
    "    ydata = []\n",
    "    for f in files:\n",
    "        f_name = f[0]\n",
    "        atrributes = f[1]\n",
    "        print(\"FILE: \", f)\n",
    "        if ( f==files[2] ):\n",
    "            class_att = f[1]\n",
    "            y = Leer_Datos(f_name,class_att)\n",
    "            print('y.shape')\n",
    "            print(y.shape)\n",
    "            ydata.append(y.values)\n",
    "           \n",
    "        data = Leer_Datos(f_name,atrributes)\n",
    "        print('data.shape')\n",
    "        print(data.shape)\n",
    "        data = data.replace(to_replace='female',value=1,regex=True)\n",
    "        data = data.replace(to_replace='male',value=0,regex=True)\n",
    "        data = data.replace(to_replace='C',value=0,regex=True)\n",
    "        data = data.replace(to_replace='S',value=1,regex=True)\n",
    "        data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "        data = data.values[~np.isnan(data).any(axis=1)]\n",
    "       \n",
    "        data[:,1:3] = Normalizar_Datos(data[:,1:3])\n",
    "        Xdata.append(data)\n",
    "        #print(Xdata)\n",
    "        \n",
    "    nb_iteration = 1000\n",
    "    learn_rate = 0.01\n",
    "    #result_tb = [learn_rate]\n",
    "    #print(\"m: \", 0)\n",
    "    #print(Xdata)\n",
    "    #print(ydata)\n",
    "    data = np.concatenate((Xdata, ydata), axis=1)\n",
    "    #print(data)\n",
    "    set_train, set_test = train_test(data)\n",
    "    X_train, y_train = divide_X_y(set_train)\n",
    "    X_test, y_test = divide_X_y(set_test)\n",
    "        \n",
    "    X_train, X_test = add_ones(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    n_features = X_train.shape[1]\n",
    "    #theta = np.zeros(n_features)\n",
    "    theta = np.random.rand(n_features)\n",
    "    theta = theta.reshape(-1,1)\n",
    "    #print(theta)\n",
    "    theta_gd, cost_history = gradient_descent(X_train, y_train, theta, nb_iteration, learn_rate)\n",
    "    theta_gd1, cost_history1 = gradient_descent(X_test, y_test, theta, nb_iteration, learn_rate)\n",
    "    error_train = calcular_funcion_costo(X_train, y_train, theta_gd)\n",
    "    error_test = calcular_funcion_costo(X_test, y_test, theta_gd)\n",
    "       \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(range(len(cost_history)), cost_history)\n",
    "    plt.plot(range(len(cost_history1)), cost_history1)\n",
    "    plt.title('TRAIN - TEST '+str(nb_iteration), {'fontsize':10})\n",
    "    print(\"Weights of gradient_descent - training data: \", theta_gd, \"\\n\")\n",
    "    print(\"Weights of gradient_descent - testing data: \", theta_gd1, \"\\n\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "GD_ploteo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass1():\n",
    "    #data = Leer_Datos('Iris.csv')\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cant_x_class = data[data.columns[data.shape[1]-1]].value_counts()\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = []    \n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    data = np.array(data)     \n",
    "    X, y = divide_X_y(data)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada = np.concatenate((X_normalizada, y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)   \n",
    "    \n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        for j in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[i], categories_train[j]), axis=0)\n",
    "            #tmp_data = np.concatenate([np.expand_dims(i,axis=0) for i in [y_classes[i],y_classes[j]]])\n",
    "            \n",
    "            posiciones_a_cambiar = np.where(tmp_data == y_classes[i]) \n",
    "            tmp_data = np.c_[tmp_data[:, :tmp_data.shape[1]-1], np.zeros(tmp_data.shape[0])]\n",
    "            tmp_data[posiciones_a_cambiar] = 1\n",
    "            np.random.shuffle(tmp_data)\n",
    "            \n",
    "            set_train, set_test = train_test(tmp_data)\n",
    "            X_train, y_train = divide_X_y(set_train)\n",
    "            X_test, y_test = divide_X_y(set_test)\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "            theta = np.random.rand(X_train.shape[1])\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "            \n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "            \n",
    "    #print(W_array)\n",
    "    print('Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 1.0, 0.7]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = [] \n",
    "    accuracy_array_test = []\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)  \n",
    "    # k_set_X[0], k_set_y[0] return first element of e/array\n",
    "    #k_set_X, k_set_y = create_k_folds(pd.DataFrame(data_normalizada), k)    \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        for c1 in range(c+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[c], categories_train[c1]), axis=0)\n",
    "            \n",
    "            total_accuracy_test = 0\n",
    "            for i_test in range(0, k): \n",
    "                X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "                y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "                X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "                y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "                \n",
    "                for j in range(0, k): \n",
    "                    if (i_test == j):\n",
    "                        X_test = k_set[i_test]['X']\n",
    "                        y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                        #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                    else:\n",
    "                        X_train = k_set[j]['X']\n",
    "                        y_train = k_set[j]['y'] == y_classes[c]  \n",
    "                \n",
    "                y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                theta = np.random.rand(np.size(X_train[0]))\n",
    "                X_train = X_train.astype(float)\n",
    "                y_train = y_train.astype(int)\n",
    "                W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "                W_array.append(W)\n",
    "\n",
    "                X_test = X_test.astype('float')\n",
    "                y_test = y_test.astype(int)\n",
    "                accuracy_test = accuracy(X_test, y_test, W)\n",
    "                total_accuracy_test += accuracy_test\n",
    "\n",
    "            total_accuracy_test = total_accuracy_test / k\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.78125, 0.96875]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) \n",
    "\n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "\n",
    "    for c in range(y_classes.shape[0]):  \n",
    "        tmp_data = np.c_[train[:, :train.shape[1]-1], np.zeros(train.shape[0])]\n",
    "        tmp_data[np.where(train == y_classes[c])] = 1\n",
    "        \n",
    "        set_train, set_test = train_test(tmp_data)\n",
    "        X_train, y_train = divide_X_y(set_train)\n",
    "        X_test, y_test = divide_X_y(set_test)\n",
    "        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "        theta = np.random.rand(X_train.shape[1])\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "        W_array.append(W)\n",
    "            \n",
    "        X_test = X_test.astype('float')\n",
    "        y_test = y_test.astype(int)\n",
    "        accuracy_test = accuracy(X_test, y_test, W)\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "        \n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.7, 0.9533333333333334]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "        \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "   \n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        total_accuracy_test = 0\n",
    "        for i_test in range(0, k): \n",
    "            X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "            y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "            X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "            y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "     \n",
    "            for j in range(0, k): \n",
    "                if (i_test == j):\n",
    "                    X_test = k_set[i_test]['X']\n",
    "                    y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                    #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                else:\n",
    "                    X_train = k_set[j]['X']\n",
    "                    y_train = k_set[j]['y'] == y_classes[c]                   \n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "            \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "          \n",
    "            theta = np.random.rand(np.size(X_train[0]))\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "\n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            total_accuracy_test += accuracy_test\n",
    "            \n",
    "        total_accuracy_test = total_accuracy_test / k\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2_cross_validation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}