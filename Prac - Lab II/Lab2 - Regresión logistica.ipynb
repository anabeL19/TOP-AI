{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leer_Datos(filename,atrributes):\n",
    "    data = pd.read_csv(filename, usecols=atrributes) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = data.mean(axis=0)\n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(theta,X):\n",
    "    return 1 / (1 + np.exp( -np.dot(X,theta) ) )        \n",
    "    #return 1 / (1 + np.exp( -np.dot(X,theta.T) ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data): \n",
    "    porcentage = 0.70\n",
    "    np.random.shuffle(data)\n",
    "    rows = int(porcentage * len(data))\n",
    "    #int((70*len(data))/100)\n",
    "    train = data[:rows, :]\n",
    "    test = data[rows:, :]\n",
    "    #X_train = data[:rows, :col]\n",
    "    #y_train = data[:rows, col]\n",
    "    #X_test = data[rows:, :col]\n",
    "    #y_test = data[rows:, col]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_X_y(data):\n",
    "    col = data.shape[1]-1\n",
    "    X = data[:, :col]\n",
    "    y = data[:, col:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_funcion_costo(X,y, theta):\n",
    "    m = y.shape[0]\n",
    "    predictions = Sigmoidal(theta,X)\n",
    "    error = (y * np.log(predictions)) - ((1-y) * np.log(1-predictions))\n",
    "    #error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\n",
    "    return -1/m * (np.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "    for i in range(nro_iter):\n",
    "        pred = Sigmoidal(theta, X)\n",
    "        pred = pred - y   \n",
    "        cost_history[i] = calcular_funcion_costo( X, y, theta) \n",
    "        theta = theta - (learning_rate * ((np.matmul(X.T,pred))/m))\n",
    "        #theta = theta - (learning_rate * ((np.matmul(pred,X))/m))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predict = Sigmoidal(theta, X)\n",
    "    #print(\"predict: \",predict)\n",
    "    probab_threshold = 0.5  \n",
    "    predicted_classes = (predict >= probab_threshold)\n",
    "    result = np.logical_xor(np.logical_not(predicted_classes), y)\n",
    "    return np.sum(result) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    col_class = data[:,-1]\n",
    "    #col_class = np.array([0,0,1,0,1,0,1,0,0,0])\n",
    "    num_rows = col_class.shape[0]\n",
    "    unique, counts = np.unique(col_class, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "\n",
    "    percent_of_first_class = round((distribution[0.0] * 100) /  num_rows)\n",
    "   \n",
    "    percent_of_second_class = round((distribution[1.0] * 100) / num_rows)\n",
    "   \n",
    "    num_per_fold = round(num_rows/k)\n",
    "    \n",
    "    num_first_class_per_fold = round((num_per_fold * percent_of_first_class)/100)\n",
    "    num_second_class_per_fold = round((num_per_fold * percent_of_second_class)/100)\n",
    "\n",
    "    list_indices=[]\n",
    "    num_0s=[]\n",
    "    num_1s=[]\n",
    "    \n",
    "    for i in range(k):\n",
    "        list_indices.append([])\n",
    "        num_0s.append(0)\n",
    "        num_1s.append(0)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "    \n",
    "        if col_class[i] == 0.0:\n",
    "            for j in range(k):\n",
    "                if(num_0s[j]<num_first_class_per_fold):\n",
    "                    num_0s[j]+=1\n",
    "                    #print(list_indices[j],list_indices[j].count(0))\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for j in range(k):\n",
    "                if(num_1s[j]<num_second_class_per_fold):\n",
    "                    num_1s[j]+=1\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "    if num_rows%k!=0:\n",
    "        list_indices[k-1].append(col_class.shape[0]-1)\n",
    "    \n",
    "    return list_indices\n",
    "    \n",
    "    #print(list_indices[0])\n",
    "    #print(list_indices[1])\n",
    "    #print(list_indices[2])\n",
    "\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n(46695, 4)\n(46695, 4)\n(46462, 4)\n"
     ]
    }
   ],
   "source": [
    "def GD_find_parameters():\n",
    "    files = [(\"Data_classification/weatherAUS.csv\",[\"MinTemp\",\"MaxTemp\",\"RainToday\"],[\"RainTomorrow\"])]\n",
    "    for f in files:\n",
    "        f_name = f[0]\n",
    "        atrributes = f[1]\n",
    "        class_att = f[2]\n",
    "        k = 3\n",
    "        data = Leer_Datos(f_name,atrributes)\n",
    "        data = data.replace(to_replace='Yes',value=1,regex=True)\n",
    "        data = data.replace(to_replace='No',value=0,regex=True)\n",
    "\n",
    "        data = data.values[~np.isnan(data).any(axis=1)]\n",
    "        data[:,0:2] = Normalizar_Datos(data[:,0:2])\n",
    "        \n",
    "        indices = create_k_folds(data, k)\n",
    "       \n",
    "\n",
    "        fold1 = data[indices[0]]\n",
    "        fold2 = data[indices[1]]\n",
    "        fold3 = data[indices[2]]\n",
    "\n",
    "        nb_iterations = [500,1000,1500,2000,2500,3000,3500]\n",
    "        nb_its_label = nb_iterations.copy()\n",
    "        learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        result_tb = [learning_rate]\n",
    "        print(\"FILE: \", f)\n",
    "        \n",
    "        for nb_it in nb_iterations:\n",
    "            rlearning_rate = []\n",
    "            for learn_rate in learning_rate:\n",
    "                #print(\"num_iter = \", nb_it)\n",
    "                #print(\"learn_rate = \", learn_rate)\n",
    "                accuracy_total = 0.0\n",
    "                for i in range(0,len(set_X)):\n",
    "                    X_train = np.zeros((set_X[i].shape[0], set_X[i].shape[1] - 1))\n",
    "                    y_train = np.zeros((set_X[i].shape[0], 1))\n",
    "                    X_test = np.c_[set_X[i], np.ones(set_X[i].shape[0])]        #bias\n",
    "                    y_test = set_y[i]\n",
    "                    for t in range(0,k):\n",
    "                        if t!=i:\n",
    "                            #print(\"set_X[t]: \",set_X[t])\n",
    "                            X_train = np.c_[set_X[t], np.ones(set_X[t].shape[0])]        #bias\n",
    "                            #print(\"X_train: \",X_train)\n",
    "                            y_train = set_y[t]\n",
    "                            #set_X_train.append(X_train)\n",
    "                            #set_y_train.append(y_train)\n",
    "                    theta = create_theta(X_train)\n",
    "                    theta, cost_history = gradient_descent(X_train, y_train, theta, nb_it, learn_rate)\n",
    "                    accuracy_train = calculate_accuracy(X_train, y_train, theta)\n",
    "                    #print(\"accuracy train: \",accuracy_train) \n",
    "                    cost_train = calculate_cost_function(X_train, y_train, theta)\n",
    "                    #print(\"Pesos de Gradiente descendiente: \", theta)\n",
    "                    #print(\"Cost training: \", cost_train)\n",
    "                    cost_test = calculate_cost_function(X_test, y_test, theta)\n",
    "                    accuracy_test = calculate_accuracy(X_test, y_test, theta) \n",
    "                \n",
    "                    accuracy_total += accuracy_test\n",
    "                accuracy_total /= k\n",
    "                rlearning_rate.append(accuracy_total)\n",
    "                #print(\"Pesos de Gradiente descendiente: \", theta)\n",
    "                #print(\"accuracy test: \",accuracy_test) \n",
    "                #print(\"Costo test: \", cost_test, \"\\n\")\n",
    "            result_tb.append(rlearning_rate)\n",
    "\n",
    "        m = np.asarray(result_tb)\n",
    "        pdObj = pd.DataFrame(m.T[:], columns=['TL|It','500','1000','1500','2000','2500','3000','3500']) \n",
    "        print(pdObj)\n",
    "        \n",
    "GD_find_parameters()"
   ]
  },
  {
   "source": [
    "#### EXPERIMENTO II"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass1():\n",
    "    #data = Leer_Datos('Iris.csv')\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cant_x_class = data[data.columns[data.shape[1]-1]].value_counts()\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = []    \n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    data = np.array(data)     \n",
    "    X, y = divide_X_y(data)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada = np.concatenate((X_normalizada, y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)   \n",
    "    \n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        for j in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[i], categories_train[j]), axis=0)\n",
    "            #tmp_data = np.concatenate([np.expand_dims(i,axis=0) for i in [y_classes[i],y_classes[j]]])\n",
    "            \n",
    "            posiciones_a_cambiar = np.where(tmp_data == y_classes[i]) \n",
    "            tmp_data = np.c_[tmp_data[:, :tmp_data.shape[1]-1], np.zeros(tmp_data.shape[0])]\n",
    "            tmp_data[posiciones_a_cambiar] = 1\n",
    "            np.random.shuffle(tmp_data)\n",
    "            \n",
    "            set_train, set_test = train_test(tmp_data)\n",
    "            X_train, y_train = divide_X_y(set_train)\n",
    "            X_test, y_test = divide_X_y(set_test)\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "            theta = np.random.rand(X_train.shape[1])\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "            \n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "            \n",
    "    #print(W_array)\n",
    "    print('Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 1.0, 0.7]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = [] \n",
    "    accuracy_array_test = []\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)  \n",
    "    # k_set_X[0], k_set_y[0] return first element of e/array\n",
    "    #k_set_X, k_set_y = create_k_folds(pd.DataFrame(data_normalizada), k)    \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        for c1 in range(c+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[c], categories_train[c1]), axis=0)\n",
    "            \n",
    "            total_accuracy_test = 0\n",
    "            for i_test in range(0, k): \n",
    "                X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "                y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "                X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "                y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "                \n",
    "                for j in range(0, k): \n",
    "                    if (i_test == j):\n",
    "                        X_test = k_set[i_test]['X']\n",
    "                        y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                        #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                    else:\n",
    "                        X_train = k_set[j]['X']\n",
    "                        y_train = k_set[j]['y'] == y_classes[c]  \n",
    "                \n",
    "                y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                theta = np.random.rand(np.size(X_train[0]))\n",
    "                X_train = X_train.astype(float)\n",
    "                y_train = y_train.astype(int)\n",
    "                W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "                W_array.append(W)\n",
    "\n",
    "                X_test = X_test.astype('float')\n",
    "                y_test = y_test.astype(int)\n",
    "                accuracy_test = accuracy(X_test, y_test, W)\n",
    "                total_accuracy_test += accuracy_test\n",
    "\n",
    "            total_accuracy_test = total_accuracy_test / k\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.78125, 0.96875]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) \n",
    "\n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "\n",
    "    for c in range(y_classes.shape[0]):  \n",
    "        tmp_data = np.c_[train[:, :train.shape[1]-1], np.zeros(train.shape[0])]\n",
    "        tmp_data[np.where(train == y_classes[c])] = 1\n",
    "        \n",
    "        set_train, set_test = train_test(tmp_data)\n",
    "        X_train, y_train = divide_X_y(set_train)\n",
    "        X_test, y_test = divide_X_y(set_test)\n",
    "        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "        theta = np.random.rand(X_train.shape[1])\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "        W_array.append(W)\n",
    "            \n",
    "        X_test = X_test.astype('float')\n",
    "        y_test = y_test.astype(int)\n",
    "        accuracy_test = accuracy(X_test, y_test, W)\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "        \n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.7, 0.9533333333333334]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "        \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "   \n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        total_accuracy_test = 0\n",
    "        for i_test in range(0, k): \n",
    "            X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "            y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "            X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "            y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "     \n",
    "            for j in range(0, k): \n",
    "                if (i_test == j):\n",
    "                    X_test = k_set[i_test]['X']\n",
    "                    y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                    #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                else:\n",
    "                    X_train = k_set[j]['X']\n",
    "                    y_train = k_set[j]['y'] == y_classes[c]                   \n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "            \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "          \n",
    "            theta = np.random.rand(np.size(X_train[0]))\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "\n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            total_accuracy_test += accuracy_test\n",
    "            \n",
    "        total_accuracy_test = total_accuracy_test / k\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2_cross_validation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}