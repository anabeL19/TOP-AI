{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leer_Datos(filename,atrributes):\n",
    "    data = pd.read_csv(filename, usecols=atrributes) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = data.mean(axis=0)\n",
    "    std_ = data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(theta,X):\n",
    "    return 1 / (1 + np.exp( -np.dot(X,theta) ) )        \n",
    "    #return 1 / (1 + np.exp( -np.dot(X,theta.T) ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data): \n",
    "    porcentage = 0.70\n",
    "    np.random.shuffle(data)\n",
    "    rows = int(porcentage * len(data))\n",
    "    #int((70*len(data))/100)\n",
    "    train = data[:rows, :]\n",
    "    test = data[rows:, :]\n",
    "    #X_train = data[:rows, :col]\n",
    "    #y_train = data[:rows, col]\n",
    "    #X_test = data[rows:, :col]\n",
    "    #y_test = data[rows:, col]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_X_y(data):\n",
    "    col = data.shape[1]-1\n",
    "    X = data[:, :col]\n",
    "    y = data[:, col:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_funcion_costo(X,y, theta):\n",
    "    m = y.shape[0]\n",
    "    predictions = Sigmoidal(theta,X) \n",
    "    #predictions = np.rint(predictions)\n",
    "    #print(predictions)\n",
    "    error = (y * np.log(predictions)) + ((1-y) * np.log(1-predictions))\n",
    "    #error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\n",
    "    return (-1/m) * (np.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "\n",
    "    for i in range(nro_iter):\n",
    "        pred = Sigmoidal(theta, X)\n",
    "        pred = pred - y   \n",
    "        cost_history[i] = calcular_funcion_costo( X, y, theta) \n",
    "        #print((learning_rate * ((np.matmul(X.T,pred))/m)).shape)\n",
    "        #print(theta.shape)\n",
    "        theta = theta - (learning_rate * ((np.matmul(X.T,pred))/m))\n",
    "        #theta = theta - (learning_rate * ((np.matmul(pred,X))/m))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predict = Sigmoidal(theta, X)\n",
    "    #print(\"predict: \",predict)\n",
    "    probab_threshold = 0.5  \n",
    "    predicted_classes = (predict >= probab_threshold)\n",
    "    result = np.logical_xor(np.logical_not(predicted_classes), y)\n",
    "    return np.sum(result) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    np.random.shuffle(data)\n",
    "    col_class = data[:,-1]\n",
    "    #col_class = np.array([0,0,1,0,1,0,1,0,0,0])\n",
    "    num_rows = col_class.shape[0]\n",
    "    unique, counts = np.unique(col_class, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    print(distribution)\n",
    "    percent_of_first_class = round((distribution[0.0] * 100) /  num_rows)\n",
    "   \n",
    "    percent_of_second_class = round((distribution[1.0] * 100) / num_rows)\n",
    "   \n",
    "    num_per_fold = round(num_rows/k)\n",
    "    \n",
    "    num_first_class_per_fold = round((num_per_fold * percent_of_first_class)/100)\n",
    "    num_second_class_per_fold = round((num_per_fold * percent_of_second_class)/100)\n",
    "\n",
    "    list_indices=[]\n",
    "    num_0s=[]\n",
    "    num_1s=[]\n",
    "    \n",
    "    for i in range(k):\n",
    "        list_indices.append([])\n",
    "        num_0s.append(0)\n",
    "        num_1s.append(0)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "    \n",
    "        if col_class[i] == 0.0:\n",
    "            for j in range(k):\n",
    "                if(num_0s[j]<num_first_class_per_fold):\n",
    "                    num_0s[j]+=1\n",
    "                    #print(list_indices[j],list_indices[j].count(0))\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for j in range(k):\n",
    "                if(num_1s[j]<num_second_class_per_fold):\n",
    "                    num_1s[j]+=1\n",
    "                    list_indices[j].append(i)\n",
    "                    break\n",
    "\n",
    "    if num_rows%k!=0:\n",
    "        list_indices[k-1].append(col_class.shape[0]-1)\n",
    "    \n",
    "    return list_indices\n",
    "    \n",
    "    #print(list_indices[0])\n",
    "    #print(list_indices[1])\n",
    "    #print(list_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareXandY(training,test):\n",
    "    train_X = training[:,:-1]\n",
    "    train_X = np.concatenate((np.ones([train_X.shape[0], 1]), train_X), axis=1)\n",
    "\n",
    "    test_X = test[:,:-1]\n",
    "    test_X = np.concatenate((np.ones([test_X.shape[0], 1]), test_X), axis=1)\n",
    "\n",
    "    train_y = training[:,-1]\n",
    "    test_y = test[:,-1]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tasa:  0.01  iteracion:  500\n",
      "[[ 1.          0.15876268 -0.89018049  1.        ]\n",
      " [ 1.          0.40860619  0.34664365  0.        ]\n",
      " [ 1.          1.12690627  0.20609545  0.        ]\n",
      " ...\n",
      " [ 1.         -1.05922442 -0.79179675  0.        ]\n",
      " [ 1.         -0.24723302  0.72612378  0.        ]\n",
      " [ 1.         -0.09108083  0.75423342  0.        ]]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[[ 1.          0.03384093 -0.70746783  1.        ]\n",
      " [ 1.         -0.1535417  -0.48259071  1.        ]\n",
      " [ 1.          0.08068658 -0.9042353   1.        ]\n",
      " ...\n",
      " [ 1.         -1.05922442 -0.79179675  0.        ]\n",
      " [ 1.         -0.24723302  0.72612378  0.        ]\n",
      " [ 1.         -0.09108083  0.75423342  0.        ]]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[[ 1.          0.03384093 -0.70746783  1.        ]\n",
      " [ 1.         -0.1535417  -0.48259071  1.        ]\n",
      " [ 1.          0.08068658 -0.9042353   1.        ]\n",
      " ...\n",
      " [ 1.         -0.01300473  1.09154909  1.        ]\n",
      " [ 1.          1.67343894  0.71206896  1.        ]\n",
      " [ 1.         -0.2316178  -0.94639976  0.        ]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "0.7796229248128616\n",
      "tasa:  0.01  iteracion:  1000\n",
      "[[ 1.          0.15876268 -0.89018049  1.        ]\n",
      " [ 1.          0.40860619  0.34664365  0.        ]\n",
      " [ 1.          1.12690627  0.20609545  0.        ]\n",
      " ...\n",
      " [ 1.         -1.05922442 -0.79179675  0.        ]\n",
      " [ 1.         -0.24723302  0.72612378  0.        ]\n",
      " [ 1.         -0.09108083  0.75423342  0.        ]]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[[ 1.          0.03384093 -0.70746783  1.        ]\n",
      " [ 1.         -0.1535417  -0.48259071  1.        ]\n",
      " [ 1.          0.08068658 -0.9042353   1.        ]\n",
      " ...\n",
      " [ 1.         -1.05922442 -0.79179675  0.        ]\n",
      " [ 1.         -0.24723302  0.72612378  0.        ]\n",
      " [ 1.         -0.09108083  0.75423342  0.        ]]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[[ 1.          0.03384093 -0.70746783  1.        ]\n",
      " [ 1.         -0.1535417  -0.48259071  1.        ]\n",
      " [ 1.          0.08068658 -0.9042353   1.        ]\n",
      " ...\n",
      " [ 1.         -0.01300473  1.09154909  1.        ]\n",
      " [ 1.          1.67343894  0.71206896  1.        ]\n",
      " [ 1.         -0.2316178  -0.94639976  0.        ]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "0.7796229248128616\n",
      "tasa:  0.01  iteracion:  1500\n",
      "[[ 1.          0.15876268 -0.89018049  1.        ]\n",
      " [ 1.          0.40860619  0.34664365  0.        ]\n",
      " [ 1.          1.12690627  0.20609545  0.        ]\n",
      " ...\n",
      " [ 1.         -1.05922442 -0.79179675  0.        ]\n",
      " [ 1.         -0.24723302  0.72612378  0.        ]\n",
      " [ 1.         -0.09108083  0.75423342  0.        ]]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-7e42855b3fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mGD_find_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-78-7e42855b3fd7>\u001b[0m in \u001b[0;36mGD_find_parameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mtheta_test3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mtheta_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mtheta_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mtheta_test3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_test3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-173b9d4e1ebb>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, theta, nro_iter, learning_rate)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSigmoidal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mcost_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcular_funcion_costo\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#print((learning_rate * ((np.matmul(X.T,pred))/m)).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(theta.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-b998a8ec10e6>\u001b[0m in \u001b[0;36mcalcular_funcion_costo\u001b[1;34m(X, y, theta)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#predictions = np.rint(predictions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(predictions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def GD_find_parameters():\n",
    "    files = [(\"Data_classification/weatherAUS.csv\",[\"MinTemp\",\"MaxTemp\",\"RainToday\",\"RainTomorrow\"])]\n",
    "    for f in files:\n",
    "        f_name = f[0]\n",
    "        atrributes = f[1]\n",
    "        k = 3\n",
    "        data = Leer_Datos(f_name,atrributes)\n",
    "        data = data.replace(to_replace='Yes',value=1,regex=True)\n",
    "        data = data.replace(to_replace='No',value=0,regex=True)\n",
    "\n",
    "        data = data.values[~np.isnan(data).any(axis=1)]\n",
    "        data[:,0:2] = Normalizar_Datos(data[:,0:2])\n",
    "      \n",
    "        indices = create_k_folds(data, k)\n",
    "        #print(indices)\n",
    "\n",
    "        fold1 = data[indices[0]]\n",
    "        fold2 = data[indices[1]]\n",
    "        fold3 = data[indices[2]]\n",
    "\n",
    "\n",
    "        learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        \n",
    "        training1 = np.concatenate((fold1,fold2))\n",
    "        training2 = np.concatenate((fold3,fold2))\n",
    "        training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "        train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "        train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "        train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "\n",
    "        for tasa in learning_rate:\n",
    "            for ite in range(500,3501,500):\n",
    "                print(\"tasa: \", tasa, \" iteracion: \", ite)\n",
    "                theta_test1 = np.zeros(train_X1.shape[1])\n",
    "                theta_test2 = np.zeros(train_X2.shape[1])\n",
    "                theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "                theta_test1, cost_history1 = gradient_descent(train_X1, train_y1, theta_test1, ite, tasa)\n",
    "                theta_test2, cost_history2 = gradient_descent(train_X2, train_y2, theta_test2, ite, tasa) \n",
    "                theta_test3, cost_history3 = gradient_descent(train_X3, train_y3, theta_test3, ite, tasa) \n",
    "                \n",
    "                \n",
    "                accuracy1 = accuracy(test_X1 , test_y1, theta_test1)\n",
    "                accuracy2 = accuracy(test_X2 , test_y2, theta_test2)\n",
    "                accuracy3 = accuracy(test_X3 , test_y3, theta_test3)\n",
    "\n",
    "                print((accuracy1+accuracy2+accuracy3)/3)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "GD_find_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tasa:  0.01  iteracion:  500\n",
      "0.7382655934280699\n",
      "tasa:  0.01  iteracion:  1000\n",
      "0.8139802577097618\n",
      "tasa:  0.01  iteracion:  1500\n",
      "0.8264544480881558\n",
      "tasa:  0.01  iteracion:  2000\n",
      "0.8312492408934822\n",
      "tasa:  0.01  iteracion:  2500\n",
      "0.8369991277176013\n",
      "tasa:  0.01  iteracion:  3000\n",
      "0.8379569821237315\n",
      "tasa:  0.01  iteracion:  3500\n",
      "0.8417911601355903\n",
      "tasa:  0.05  iteracion:  500\n",
      "0.8369991277176013\n",
      "tasa:  0.05  iteracion:  1000\n",
      "0.8427490145417206\n",
      "tasa:  0.05  iteracion:  1500\n",
      "0.8446702441286561\n",
      "tasa:  0.05  iteracion:  2000\n",
      "0.8446702441286561\n",
      "tasa:  0.05  iteracion:  2500\n",
      "0.8446702441286561\n",
      "tasa:  0.05  iteracion:  3000\n",
      "0.8446702441286561\n",
      "tasa:  0.05  iteracion:  3500\n",
      "0.8446702441286561\n",
      "tasa:  0.1  iteracion:  500\n",
      "0.8427490145417206\n",
      "tasa:  0.1  iteracion:  1000\n",
      "0.8446702441286561\n",
      "tasa:  0.1  iteracion:  1500\n",
      "0.8446702441286561\n",
      "tasa:  0.1  iteracion:  2000\n",
      "0.8446702441286561\n",
      "tasa:  0.1  iteracion:  2500\n",
      "0.8446702441286561\n",
      "tasa:  0.1  iteracion:  3000\n",
      "0.8456280985347865\n",
      "tasa:  0.1  iteracion:  3500\n",
      "0.8456280985347865\n",
      "tasa:  0.2  iteracion:  500\n",
      "0.8446702441286561\n",
      "tasa:  0.2  iteracion:  1000\n",
      "0.8446702441286561\n",
      "tasa:  0.2  iteracion:  1500\n",
      "0.8456280985347865\n",
      "tasa:  0.2  iteracion:  2000\n",
      "0.8456280985347865\n",
      "tasa:  0.2  iteracion:  2500\n",
      "0.8456280985347865\n",
      "tasa:  0.2  iteracion:  3000\n",
      "0.8456280985347865\n",
      "tasa:  0.2  iteracion:  3500\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  500\n",
      "0.8446702441286561\n",
      "tasa:  0.3  iteracion:  1000\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  1500\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  2000\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  2500\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  3000\n",
      "0.8456280985347865\n",
      "tasa:  0.3  iteracion:  3500\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  500\n",
      "0.8446702441286561\n",
      "tasa:  0.4  iteracion:  1000\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  1500\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  2000\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  2500\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  3000\n",
      "0.8456280985347865\n",
      "tasa:  0.4  iteracion:  3500\n",
      "0.8456280985347865\n"
     ]
    }
   ],
   "source": [
    "def GD_find_parameters1():\n",
    "    files = [ (\"Data_classification/titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"Data_classification/gender_submission.csv\",[\"Survived\"]),\n",
    "              (\"Data_classification/titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]) ]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "    k = 3\n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='female',value=1,regex=True)\n",
    "    data = data.replace(to_replace='male',value=0,regex=True)\n",
    "    data = data.replace(to_replace='C',value=0,regex=True)\n",
    "    data = data.replace(to_replace='S',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    f_name = files[1][0]\n",
    "    atrributes = files[1][1]\n",
    "    data2 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = np.concatenate((data, data2), axis=1)\n",
    "\n",
    "    f_name = files[2][0]\n",
    "    atrributes = files[2][1]\n",
    "    data3 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data3 = data3[[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]]\n",
    "\n",
    "    data3 = data3.replace(to_replace='female',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='male',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='C',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='S',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "\n",
    "    \n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data[:,1:3] = Normalizar_Datos(data[:,1:3])\n",
    "      \n",
    "    indices = create_k_folds(data, k)\n",
    "        #print(indices)\n",
    "\n",
    "    fold1 = data[indices[0]]\n",
    "    fold2 = data[indices[1]]\n",
    "    fold3 = data[indices[2]]\n",
    "\n",
    "\n",
    "    learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        \n",
    "    training1 = np.concatenate((fold1,fold2))\n",
    "    training2 = np.concatenate((fold3,fold2))\n",
    "    training3 = np.concatenate((fold3,fold1))\n",
    "        \n",
    "    train_X1, train_y1, test_X1, test_y1 = PrepareXandY(training1,fold3)\n",
    "    train_X2, train_y2, test_X2, test_y2 = PrepareXandY(training2,fold1)\n",
    "    train_X3, train_y3, test_X3, test_y3 = PrepareXandY(training3,fold2)\n",
    "\n",
    "    for tasa in learning_rate:\n",
    "        for ite in range(500,3501,500):\n",
    "            print(\"tasa: \", tasa, \" iteracion: \", ite)\n",
    "            theta_test1 = np.zeros(train_X1.shape[1])\n",
    "            theta_test2 = np.zeros(train_X2.shape[1])\n",
    "            theta_test3 = np.zeros(train_X3.shape[1])\n",
    "                \n",
    "            theta_test1, cost_history1 = gradient_descent(train_X1, train_y1, theta_test1, ite, tasa)\n",
    "            theta_test2, cost_history2 = gradient_descent(train_X2, train_y2, theta_test2, ite, tasa) \n",
    "            theta_test3, cost_history3 = gradient_descent(train_X3, train_y3, theta_test3, ite, tasa) \n",
    "                \n",
    "                \n",
    "            accuracy1 = accuracy(test_X1 , test_y1, theta_test1)\n",
    "            accuracy2 = accuracy(test_X2 , test_y2, theta_test2)\n",
    "            accuracy3 = accuracy(test_X3 , test_y3, theta_test3)\n",
    "\n",
    "            print((accuracy1+accuracy2+accuracy3)/3)\n",
    " \n",
    "\n",
    "        \n",
    "GD_find_parameters1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.          0.          0.32639308 -0.51633622  2.        ]\n [ 1.          1.          1.19690507 -0.53121592  1.        ]\n [ 1.          0.          2.24151945 -0.48298967  2.        ]\n ...\n [ 1.          1.         -0.75304177 -0.11848896  1.        ]\n [ 1.          0.         -0.26555506 -0.11848896  0.        ]\n [ 1.          0.          0.15229069 -0.51775743  2.        ]]\n[0. 1. 0. ... 1. 1. 0.]\nWeights of gradient_descent - training data:  [-0.75336614  2.20222687 -0.16883034  0.43051355 -0.55687642] \n\n[0.69314718 0.68784436 0.68278639 0.67795562 0.67333577 0.66891186\n 0.66467007 0.6605977  0.65668305 0.65291537 0.6492848  0.64578224\n 0.64239936 0.63912849 0.63596257 0.63289512 0.62992018 0.62703225\n 0.62422629 0.62149762 0.61884196 0.61625535 0.61373412 0.61127488\n 0.6088745  0.60653009 0.60423896 0.6019986  0.5998067  0.59766111\n 0.59555982 0.59350095 0.59148276 0.58950362 0.58756199 0.58565646\n 0.58378568 0.58194839 0.5801434  0.57836962 0.57662597 0.57491149\n 0.57322522 0.5715663  0.56993388 0.56832716 0.56674541 0.5651879\n 0.56365396 0.56214293 0.56065421 0.5591872  0.55774135 0.55631612\n 0.554911   0.55352549 0.55215914 0.55081147 0.54948208 0.54817053\n 0.54687643 0.54559939 0.54433905 0.54309504 0.54186703 0.54065468\n 0.53945766 0.53827568 0.53710843 0.53595562 0.53481697 0.53369221\n 0.53258108 0.53148332 0.53039868 0.52932693 0.52826783 0.52722115\n 0.52618668 0.5251642  0.5241535  0.52315438 0.52216664 0.5211901\n 0.52022456 0.51926985 0.51832578 0.51739218 0.51646889 0.51555574\n 0.51465257 0.51375922 0.51287554 0.51200138 0.51113659 0.51028104\n 0.50943458 0.50859707 0.50776839 0.5069484  0.50613698 0.505334\n 0.50453933 0.50375287 0.50297448 0.50220406 0.5014415  0.50068669\n 0.49993951 0.49919986 0.49846764 0.49774275 0.49702508 0.49631455\n 0.49561105 0.49491449 0.49422477 0.49354182 0.49286554 0.49219583\n 0.49153263 0.49087584 0.49022537 0.48958116 0.48894312 0.48831117\n 0.48768524 0.48706524 0.48645112 0.48584278 0.48524017 0.48464321\n 0.48405183 0.48346597 0.48288556 0.48231052 0.48174081 0.48117635\n 0.48061708 0.48006294 0.47951387 0.47896981 0.4784307  0.47789649\n 0.47736711 0.47684251 0.47632264 0.47580744 0.47529686 0.47479084\n 0.47428934 0.4737923  0.47329967 0.47281141 0.47232746 0.47184778\n 0.47137232 0.47090103 0.47043387 0.46997079 0.46951174 0.46905669\n 0.46860559 0.46815839 0.46771506 0.46727556 0.46683983 0.46640785\n 0.46597957 0.46555495 0.46513396 0.46471656 0.4643027  0.46389235\n 0.46348548 0.46308204 0.46268201 0.46228534 0.46189201 0.46150198\n 0.46111521 0.46073167 0.46035133 0.45997416 0.45960012 0.45922918\n 0.45886131 0.45849648 0.45813466 0.45777582 0.45741993 0.45706696\n 0.45671688 0.45636966 0.45602528 0.4556837  0.4553449  0.45500885\n 0.45467553 0.45434491 0.45401695 0.45369165 0.45336896 0.45304887\n 0.45273134 0.45241637 0.45210391 0.45179395 0.45148646 0.45118143\n 0.45087882 0.45057861 0.45028078 0.44998531 0.44969218 0.44940135\n 0.44911283 0.44882657 0.44854256 0.44826078 0.44798121 0.44770382\n 0.4474286  0.44715553 0.44688459 0.44661575 0.446349   0.44608432\n 0.44582169 0.4455611  0.44530251 0.44504592 0.44479131 0.44453866\n 0.44428794 0.44403915 0.44379227 0.44354728 0.44330415 0.44306289\n 0.44282346 0.44258585 0.44235005 0.44211604 0.44188381 0.44165333\n 0.4414246  0.44119759 0.4409723  0.4407487  0.44052679 0.44030654\n 0.44008795 0.439871   0.43965567 0.43944195 0.43922982 0.43901928\n 0.43881031 0.43860289 0.43839702 0.43819267 0.43798984 0.43778851\n 0.43758867 0.43739031 0.43719341 0.43699796 0.43680395 0.43661137\n 0.4364202  0.43623044 0.43604207 0.43585507 0.43566945 0.43548517\n 0.43530225 0.43512066 0.43494039 0.43476143 0.43458377 0.4344074\n 0.4342323  0.43405848 0.43388591 0.43371459 0.43354451 0.43337565\n 0.43320801 0.43304158 0.43287634 0.43271228 0.43254941 0.4323877\n 0.43222714 0.43206774 0.43190947 0.43175233 0.43159632 0.43144141]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.116562pt\" version=\"1.1\" viewBox=\"0 0 378.465625 262.116562\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.116562 \r\nL 378.465625 262.116562 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 238.238437 \r\nL 371.265625 238.238437 \r\nL 371.265625 20.798437 \r\nL 36.465625 20.798437 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 51.683807 238.238437 \r\nL 51.683807 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m84ac588d4a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 102.580736 238.238437 \r\nL 102.580736 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.580736\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(96.218236 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 153.477665 238.238437 \r\nL 153.477665 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"153.477665\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(143.933915 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 204.374594 238.238437 \r\nL 204.374594 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.374594\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(194.830844 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 255.271523 238.238437 \r\nL 255.271523 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"255.271523\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.727773 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 306.168453 238.238437 \r\nL 306.168453 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.168453\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(296.624703 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 357.065382 238.238437 \r\nL 357.065382 20.798437 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"357.065382\" xlink:href=\"#m84ac588d4a\" y=\"238.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(347.521632 252.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 214.337046 \r\nL 371.265625 214.337046 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma3ccefa5eb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"214.337046\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.45 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 218.136265)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 176.570832 \r\nL 371.265625 176.570832 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"176.570832\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.50 -->\r\n      <g transform=\"translate(7.2 180.37005)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 138.804617 \r\nL 371.265625 138.804617 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"138.804617\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.55 -->\r\n      <g transform=\"translate(7.2 142.603836)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 101.038402 \r\nL 371.265625 101.038402 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"101.038402\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 104.837621)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 63.272188 \r\nL 371.265625 63.272188 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"63.272188\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.65 -->\r\n      <g transform=\"translate(7.2 67.071406)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_25\">\r\n      <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 36.465625 25.505973 \r\nL 371.265625 25.505973 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#ma3ccefa5eb\" y=\"25.505973\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.70 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 29.305192)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_27\">\r\n    <path clip-path=\"url(#pf4b42acdc8)\" d=\"M 51.683807 30.682074 \r\nL 53.719684 38.507833 \r\nL 55.755561 45.646112 \r\nL 57.791438 52.191526 \r\nL 60.845254 61.070135 \r\nL 63.89907 69.013132 \r\nL 66.952886 76.191918 \r\nL 70.006701 82.7397 \r\nL 73.060517 88.760342 \r\nL 76.114333 94.335275 \r\nL 79.168149 99.528812 \r\nL 83.239903 105.947293 \r\nL 87.311657 111.872425 \r\nL 91.383412 117.376357 \r\nL 95.455166 122.515067 \r\nL 99.52692 127.332827 \r\nL 103.598675 131.865301 \r\nL 108.688367 137.17377 \r\nL 113.77806 142.128504 \r\nL 118.867753 146.7675 \r\nL 123.957446 151.122284 \r\nL 129.047139 155.219454 \r\nL 135.154771 159.827827 \r\nL 141.262402 164.131478 \r\nL 147.370034 168.159093 \r\nL 153.477665 171.935422 \r\nL 160.603235 176.052161 \r\nL 167.728805 179.885915 \r\nL 174.854375 183.462535 \r\nL 181.979945 186.804633 \r\nL 190.123454 190.362442 \r\nL 198.266963 193.666004 \r\nL 206.410471 196.738236 \r\nL 215.571919 199.943065 \r\nL 224.733366 202.905997 \r\nL 233.894813 205.649323 \r\nL 244.074199 208.463781 \r\nL 254.253585 211.055234 \r\nL 265.450909 213.673255 \r\nL 276.648234 216.071406 \r\nL 288.863497 218.462132 \r\nL 302.096698 220.814255 \r\nL 315.3299 222.945558 \r\nL 329.58104 225.021054 \r\nL 344.850119 227.020444 \r\nL 356.047443 228.354801 \r\nL 356.047443 228.354801 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 238.238437 \r\nL 36.465625 20.798437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 238.238437 \r\nL 371.265625 20.798437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 238.238437 \r\nL 371.265625 238.238437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 20.798437 \r\nL 371.265625 20.798437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_14\">\r\n    <!-- TRAIN - TEST 300 -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n     <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-73\"/>\r\n     <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 4.890625 31.390625 \r\nL 31.203125 31.390625 \r\nL 31.203125 23.390625 \r\nL 4.890625 23.390625 \r\nz\r\n\" id=\"DejaVuSans-45\"/>\r\n     <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n     <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n    </defs>\r\n    <g transform=\"translate(160.345313 14.798437)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"61.083984\" xlink:href=\"#DejaVuSans-82\"/>\r\n     <use x=\"126.566406\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"194.974609\" xlink:href=\"#DejaVuSans-73\"/>\r\n     <use x=\"224.466797\" xlink:href=\"#DejaVuSans-78\"/>\r\n     <use x=\"299.271484\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"331.058594\" xlink:href=\"#DejaVuSans-45\"/>\r\n     <use x=\"367.142578\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"398.929688\" xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"460.013672\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"523.197266\" xlink:href=\"#DejaVuSans-83\"/>\r\n     <use x=\"586.673828\" xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"647.757812\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"679.544922\" xlink:href=\"#DejaVuSans-51\"/>\r\n     <use x=\"743.167969\" xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"806.791016\" xlink:href=\"#DejaVuSans-48\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf4b42acdc8\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"20.798437\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "def GD_ploteo():\n",
    "    files = [ (\"Data_classification/titanic_test.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\"]), \n",
    "              (\"Data_classification/gender_submission.csv\",[\"Survived\"]),\n",
    "              (\"Data_classification/titanic_train.csv\",[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]) ]\n",
    "    \n",
    "    f_name = files[0][0]\n",
    "    atrributes = files[0][1]\n",
    "  \n",
    "    data = Leer_Datos(f_name,atrributes)\n",
    "    data = data.replace(to_replace='female',value=1,regex=True)\n",
    "    data = data.replace(to_replace='male',value=0,regex=True)\n",
    "    data = data.replace(to_replace='C',value=0,regex=True)\n",
    "    data = data.replace(to_replace='S',value=1,regex=True)\n",
    "    data = data.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    f_name = files[1][0]\n",
    "    atrributes = files[1][1]\n",
    "    data2 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data = np.concatenate((data, data2), axis=1)\n",
    "\n",
    "    f_name = files[2][0]\n",
    "    atrributes = files[2][1]\n",
    "    data3 = Leer_Datos(f_name,atrributes)\n",
    "\n",
    "    data3 = data3[[\"Sex\",\"Age\",\"Fare\",\"Embarked\",\"Survived\"]]\n",
    "\n",
    "    data3 = data3.replace(to_replace='female',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='male',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='C',value=0,regex=True)\n",
    "    data3 = data3.replace(to_replace='S',value=1,regex=True)\n",
    "    data3 = data3.replace(to_replace='Q',value=2,regex=True)\n",
    "\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "\n",
    "    \n",
    "    data = data[~np.isnan(data).any(axis=1)]\n",
    "    data[:,1:3] = Normalizar_Datos(data[:,1:3])\n",
    "\n",
    "        \n",
    "    nb_iteration = 300\n",
    "    learn_rate = 0.1\n",
    "    \n",
    "\n",
    "    X_train, y_train, test_X1, test_y1 = PrepareXandY(data,data)\n",
    "\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    theta_gd, cost_history = gradient_descent(X_train, y_train, theta, nb_iteration, learn_rate)\n",
    "\n",
    "    error_train = calcular_funcion_costo(X_train, y_train, theta_gd)\n",
    " \n",
    "       \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(range(len(cost_history)), cost_history)\n",
    "\n",
    "    plt.title('TRAIN - TEST '+str(nb_iteration), {'fontsize':10})\n",
    "    print(\"Weights of gradient_descent - training data: \", theta_gd, \"\\n\")\n",
    "\n",
    "    print(cost_history)\n",
    "   \n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "GD_ploteo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass1():\n",
    "    #data = Leer_Datos('Iris.csv')\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cant_x_class = data[data.columns[data.shape[1]-1]].value_counts()\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = []    \n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    data = np.array(data)     \n",
    "    X, y = divide_X_y(data)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada = np.concatenate((X_normalizada, y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)   \n",
    "    \n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        for j in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[i], categories_train[j]), axis=0)\n",
    "            #tmp_data = np.concatenate([np.expand_dims(i,axis=0) for i in [y_classes[i],y_classes[j]]])\n",
    "            \n",
    "            posiciones_a_cambiar = np.where(tmp_data == y_classes[i]) \n",
    "            tmp_data = np.c_[tmp_data[:, :tmp_data.shape[1]-1], np.zeros(tmp_data.shape[0])]\n",
    "            tmp_data[posiciones_a_cambiar] = 1\n",
    "            np.random.shuffle(tmp_data)\n",
    "            \n",
    "            set_train, set_test = train_test(tmp_data)\n",
    "            X_train, y_train = divide_X_y(set_train)\n",
    "            X_test, y_test = divide_X_y(set_test)\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "            theta = np.random.rand(X_train.shape[1])\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "            \n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "            \n",
    "    #print(W_array)\n",
    "    print('Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 1.0, 0.7]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = [] \n",
    "    accuracy_array_test = []\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)  \n",
    "    # k_set_X[0], k_set_y[0] return first element of e/array\n",
    "    #k_set_X, k_set_y = create_k_folds(pd.DataFrame(data_normalizada), k)    \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        for c1 in range(c+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[c], categories_train[c1]), axis=0)\n",
    "            \n",
    "            total_accuracy_test = 0\n",
    "            for i_test in range(0, k): \n",
    "                X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "                y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "                X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "                y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "                \n",
    "                for j in range(0, k): \n",
    "                    if (i_test == j):\n",
    "                        X_test = k_set[i_test]['X']\n",
    "                        y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                        #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                    else:\n",
    "                        X_train = k_set[j]['X']\n",
    "                        y_train = k_set[j]['y'] == y_classes[c]  \n",
    "                \n",
    "                y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                theta = np.random.rand(np.size(X_train[0]))\n",
    "                X_train = X_train.astype(float)\n",
    "                y_train = y_train.astype(int)\n",
    "                W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "                W_array.append(W)\n",
    "\n",
    "                X_test = X_test.astype('float')\n",
    "                y_test = y_test.astype(int)\n",
    "                accuracy_test = accuracy(X_test, y_test, W)\n",
    "                total_accuracy_test += accuracy_test\n",
    "\n",
    "            total_accuracy_test = total_accuracy_test / k\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.78125, 0.96875]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) \n",
    "\n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "\n",
    "    for c in range(y_classes.shape[0]):  \n",
    "        tmp_data = np.c_[train[:, :train.shape[1]-1], np.zeros(train.shape[0])]\n",
    "        tmp_data[np.where(train == y_classes[c])] = 1\n",
    "        \n",
    "        set_train, set_test = train_test(tmp_data)\n",
    "        X_train, y_train = divide_X_y(set_train)\n",
    "        X_test, y_test = divide_X_y(set_test)\n",
    "        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "        theta = np.random.rand(X_train.shape[1])\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "        W_array.append(W)\n",
    "            \n",
    "        X_test = X_test.astype('float')\n",
    "        y_test = y_test.astype(int)\n",
    "        accuracy_test = accuracy(X_test, y_test, W)\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "        \n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.7, 0.9533333333333334]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "        \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "   \n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        total_accuracy_test = 0\n",
    "        for i_test in range(0, k): \n",
    "            X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "            y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "            X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "            y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "     \n",
    "            for j in range(0, k): \n",
    "                if (i_test == j):\n",
    "                    X_test = k_set[i_test]['X']\n",
    "                    y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                    #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                else:\n",
    "                    X_train = k_set[j]['X']\n",
    "                    y_train = k_set[j]['y'] == y_classes[c]                   \n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "            \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "          \n",
    "            theta = np.random.rand(np.size(X_train[0]))\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "\n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            total_accuracy_test += accuracy_test\n",
    "            \n",
    "        total_accuracy_test = total_accuracy_test / k\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2_cross_validation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}