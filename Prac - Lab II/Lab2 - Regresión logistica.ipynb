{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "- Cargar datos \n",
    "- Normalizar datos \n",
    "- Agregar una columna de unos\n",
    "- Calcular la prediccion (theta tanspuesta por X vector) \n",
    "- Calcular el costo (error)\n",
    "- Dos formas para calcular los parámetros (thetas)\n",
    "    - Ecuacion normal (X entrenaminto * producto matricil-< inversa - Xt entrenamiento Y(años que vivere))\n",
    "    - Gradiente Descendiente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Leer_Datos(filename):\n",
    "    data = pd.read_csv(filename, delim_whitespace=True)\n",
    "    #return np.array(pd)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data):\n",
    "    mean_ = np.mean(data) \n",
    "    std_ = np.std(data) #data.std(axis=0) #estandar\n",
    "    return (data - mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(theta,X):\n",
    "    return 1 / (1 + np.exp( -np.dot(X,theta) ) )        \n",
    "    #return 1 / (1 + np.exp( -np.dot(X,theta.T) ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data): \n",
    "    porcentage = 0.70\n",
    "    np.random.shuffle(data)\n",
    "    rows = int(porcentage * len(data))\n",
    "    #int((70*len(data))/100)\n",
    "    train = data[:rows, :]\n",
    "    test = data[rows:, :]\n",
    "    #X_train = data[:rows, :col]\n",
    "    #y_train = data[:rows, col]\n",
    "    #X_test = data[rows:, :col]\n",
    "    #y_test = data[rows:, col]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_X_y(data):\n",
    "    col = data.shape[1]-1\n",
    "    X = data[:, :col]\n",
    "    y = data[:, col:]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones (bias)\n",
    "def add_ones(X_train, y_train, X_test, y_test):\n",
    "    n_exa_train = len(y_train)\n",
    "    n_exa_test = len(y_test)\n",
    "\n",
    "    X_train = np.concatenate((np.ones([n_exa_train, 1]), X_train), axis=1)\n",
    "    X_test = np.concatenate((np.ones([n_exa_test, 1]), X_test), axis=1)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_funcion_costo(X,y, theta):\n",
    "    m = y.shape[0]\n",
    "    predictions = Sigmoidal(theta,X)\n",
    "    error = (y * np.log(predictions)) - ((1-y) * np.log(1-predictions))\n",
    "    #error = (y.dot(np.log(predictions))) + ((1-y) * np.log(1-predictions))\n",
    "    return -1/m * (np.sum(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, nro_iter, learning_rate): \n",
    "    m = X.shape[0] #nbr of training data\n",
    "    #cost_history = np.empty(nro_iter, dtype=float) \n",
    "    cost_history = np.zeros(nro_iter) \n",
    "    for i in range(nro_iter):\n",
    "        pred = Sigmoidal(theta, X)\n",
    "        pred = pred - y   \n",
    "        cost_history[i] = calcular_funcion_costo( X, y, theta) \n",
    "        theta = theta - (learning_rate * ((np.matmul(X.T,pred))/m))\n",
    "        #theta = theta - (learning_rate * ((np.matmul(pred,X))/m))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predict = Sigmoidal(theta, X)\n",
    "    #print(\"predict: \",predict)\n",
    "    probab_threshold = 0.5  \n",
    "    predicted_classes = (predict >= probab_threshold)\n",
    "    result = np.logical_xor(np.logical_not(predicted_classes), y)\n",
    "    return np.sum(result) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data, k):\n",
    "    name_col = data.columns[data.shape[1]-1]\n",
    "    cant_x_class = data[name_col].value_counts()\n",
    "    \n",
    "    data = np.array(data) \n",
    "    #X, y = divide_X_y(data)\n",
    "    i = 0   \n",
    "    k_fold = []\n",
    "    for c in cant_x_class:\n",
    "        while ( c%k != 0 ):\n",
    "            c=c-1\n",
    "        #k_data_X.append(np.split(X[i:c+i, :], k))\n",
    "        #k_data_y.append(np.split(y[i:c+i, :], k))\n",
    "        \n",
    "        k_data = np.split(data[i:c+i, :], k)\n",
    "        X, y = divide_X_y(data)\n",
    "        k_fold.append({\"X\": X, \"y\" : y})\n",
    "        i = c+i\n",
    "    #return k_data_X, k_data_y    \n",
    "    return k_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-e4d04c7ef586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mpdObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TL|It'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'500'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1000'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1500'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2000'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2500'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3000'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'3500'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mGD_find_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-242-e4d04c7ef586>\u001b[0m in \u001b[0;36mGD_find_parameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeer_Datos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalizar_Datos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-158-2790eb69e948>\u001b[0m in \u001b[0;36mNormalizar_Datos\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNormalizar_Datos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmean_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstd_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#data.std(axis=0) #estandar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstd_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3370\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3372\u001b[1;33m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[0;32m   3373\u001b[0m                           out=out, **kwargs)\n\u001b[0;32m   3374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\actual\\jupyter\\jenv\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "def GD_find_parameters():\n",
    "    files = [\"Data_classification/weatherAUS.csv\", \"Data_classification/weatherAUS.csv\"]\n",
    "\n",
    "    for f in files:\n",
    "        k = 3\n",
    "        data = Leer_Datos(f)\n",
    "        data = Normalizar_Datos(data.values)\n",
    "        np.random.shuffle(data)\n",
    "        #cross validation\n",
    "        #set_X_train, set_y_train, set_X_test, set_y_test = create_kfolds(data)\n",
    "        set_X, set_y = create_k_folds(data, k)\n",
    "        #set_X_train = []\n",
    "        #set_y_train = []\n",
    "        #for i in range(0,len(set_X)):\n",
    "            #X_test = np.c_[set_X[i], np.ones(set_X[i].shape[0])]        #bias\n",
    "            #y_test = set_y[i]\n",
    "            #for t in range(0,3):\n",
    "                #if t!=i:\n",
    "                    #print(\"set_X[t]: \",set_X[t])\n",
    "                    #X_train = np.c_[set_X[t], np.ones(set_X[t].shape[0])]        #bias\n",
    "                    #print(\"X_train: \",X_train)\n",
    "                    #y_train = set_y[t]\n",
    "                    #set_X_train.append(X_train)\n",
    "                    #set_y_train.append(y_train)\n",
    "\n",
    "        nb_iterations = [500,1000,1500,2000,2500,3000,3500]\n",
    "        nb_its_label = nb_iterations.copy()\n",
    "        learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "        result_tb = [learning_rate]\n",
    "        print(\"FILE: \", f)\n",
    "        \n",
    "        for nb_it in nb_iterations:\n",
    "            rlearning_rate = []\n",
    "            for learn_rate in learning_rate:\n",
    "                #print(\"num_iter = \", nb_it)\n",
    "                #print(\"learn_rate = \", learn_rate)\n",
    "                accuracy_total = 0.0\n",
    "                for i in range(0,len(set_X)):\n",
    "                    X_train = np.zeros((set_X[i].shape[0], set_X[i].shape[1] - 1))\n",
    "                    y_train = np.zeros((set_X[i].shape[0], 1))\n",
    "                    X_test = np.c_[set_X[i], np.ones(set_X[i].shape[0])]        #bias\n",
    "                    y_test = set_y[i]\n",
    "                    for t in range(0,k):\n",
    "                        if t!=i:\n",
    "                            #print(\"set_X[t]: \",set_X[t])\n",
    "                            X_train = np.c_[set_X[t], np.ones(set_X[t].shape[0])]        #bias\n",
    "                            #print(\"X_train: \",X_train)\n",
    "                            y_train = set_y[t]\n",
    "                            #set_X_train.append(X_train)\n",
    "                            #set_y_train.append(y_train)\n",
    "                    theta = create_theta(X_train)\n",
    "                    theta, cost_history = gradient_descent(X_train, y_train, theta, nb_it, learn_rate)\n",
    "                    accuracy_train = calculate_accuracy(X_train, y_train, theta)\n",
    "                    #print(\"accuracy train: \",accuracy_train) \n",
    "                    cost_train = calculate_cost_function(X_train, y_train, theta)\n",
    "                    #print(\"Pesos de Gradiente descendiente: \", theta)\n",
    "                    #print(\"Cost training: \", cost_train)\n",
    "                    cost_test = calculate_cost_function(X_test, y_test, theta)\n",
    "                    accuracy_test = calculate_accuracy(X_test, y_test, theta) \n",
    "                \n",
    "                    accuracy_total += accuracy_test\n",
    "                accuracy_total /= k\n",
    "                rlearning_rate.append(accuracy_total)\n",
    "                #print(\"Pesos de Gradiente descendiente: \", theta)\n",
    "                #print(\"accuracy test: \",accuracy_test) \n",
    "                #print(\"Costo test: \", cost_test, \"\\n\")\n",
    "            result_tb.append(rlearning_rate)\n",
    "\n",
    "        m = np.asarray(result_tb)\n",
    "        pdObj = pd.DataFrame(m.T[:], columns=['TL|It','500','1000','1500','2000','2500','3000','3500']) \n",
    "        print(pdObj)\n",
    "GD_find_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTO III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica\n",
      "[1.0, 1.0, 0.9523809523809523]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1():\n",
    "    #data = Leer_Datos('Iris.csv')\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cant_x_class = data[data.columns[data.shape[1]-1]].value_counts()\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = []    \n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    data = np.array(data)     \n",
    "    X, y = divide_X_y(data)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada = np.concatenate((X_normalizada, y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)   \n",
    "    \n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        for j in range(i+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[i], categories_train[j]), axis=0)\n",
    "            #tmp_data = np.concatenate([np.expand_dims(i,axis=0) for i in [y_classes[i],y_classes[j]]])\n",
    "            \n",
    "            posiciones_a_cambiar = np.where(tmp_data == y_classes[i]) \n",
    "            tmp_data = np.c_[tmp_data[:, :tmp_data.shape[1]-1], np.zeros(tmp_data.shape[0])]\n",
    "            tmp_data[posiciones_a_cambiar] = 1\n",
    "            np.random.shuffle(tmp_data)\n",
    "            \n",
    "            set_train, set_test = train_test(tmp_data)\n",
    "            X_train, y_train = divide_X_y(set_train)\n",
    "            X_test, y_test = divide_X_y(set_test)\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "            theta = np.random.rand(X_train.shape[1])\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "            \n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "            \n",
    "    #print(W_array)\n",
    "    print('Iris-setosa vs Iris-versicolor, Iris-setosa vs Iris-virginica, Iris-versicolor vs virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 1.0, 0.7]\n"
     ]
    }
   ],
   "source": [
    "def multiclass1_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    W_array = [] \n",
    "    accuracy_array_test = []\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    y_classes = np.unique(y)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) # verificar la division random de train and test\n",
    "    categories_train = []\n",
    "    for i in range(0, y_classes.shape[0]):\n",
    "        category = train[train[:, train.shape[1]-1] == y_classes[i]]\n",
    "        categories_train.append(category)  \n",
    "    # k_set_X[0], k_set_y[0] return first element of e/array\n",
    "    #k_set_X, k_set_y = create_k_folds(pd.DataFrame(data_normalizada), k)    \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        for c1 in range(c+1, y_classes.shape[0]):\n",
    "            tmp_data = np.concatenate((categories_train[c], categories_train[c1]), axis=0)\n",
    "            \n",
    "            total_accuracy_test = 0\n",
    "            for i_test in range(0, k): \n",
    "                X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "                y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "                X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "                y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "                \n",
    "                for j in range(0, k): \n",
    "                    if (i_test == j):\n",
    "                        X_test = k_set[i_test]['X']\n",
    "                        y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                        #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                    else:\n",
    "                        X_train = k_set[j]['X']\n",
    "                        y_train = k_set[j]['y'] == y_classes[c]  \n",
    "                \n",
    "                y_train = np.reshape(y_train, y_train.shape[0])\n",
    "                y_test = np.reshape(y_test, y_test.shape[0])\n",
    "\n",
    "                X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "                X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "\n",
    "                theta = np.random.rand(np.size(X_train[0]))\n",
    "                X_train = X_train.astype(float)\n",
    "                y_train = y_train.astype(int)\n",
    "                W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "                W_array.append(W)\n",
    "\n",
    "                X_test = X_test.astype('float')\n",
    "                y_test = y_test.astype(int)\n",
    "                accuracy_test = accuracy(X_test, y_test, W)\n",
    "                total_accuracy_test += accuracy_test\n",
    "\n",
    "            total_accuracy_test = total_accuracy_test / k\n",
    "            accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass1_cross_validation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.78125, 0.96875]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "    train, test = train_test(data_normalizada) \n",
    "\n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "\n",
    "    for c in range(y_classes.shape[0]):  \n",
    "        tmp_data = np.c_[train[:, :train.shape[1]-1], np.zeros(train.shape[0])]\n",
    "        tmp_data[np.where(train == y_classes[c])] = 1\n",
    "        \n",
    "        set_train, set_test = train_test(tmp_data)\n",
    "        X_train, y_train = divide_X_y(set_train)\n",
    "        X_test, y_test = divide_X_y(set_test)\n",
    "        y_train = np.reshape(y_train, y_train.shape[0])\n",
    "        y_test = np.reshape(y_test, y_test.shape[0])\n",
    "        \n",
    "        X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "        X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "        \n",
    "        theta = np.random.rand(X_train.shape[1])\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "        W_array.append(W)\n",
    "            \n",
    "        X_test = X_test.astype('float')\n",
    "        y_test = y_test.astype(int)\n",
    "        accuracy_test = accuracy(X_test, y_test, W)\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "        \n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "[1.0, 0.7, 0.9533333333333334]\n"
     ]
    }
   ],
   "source": [
    "def multiclass2_cross_validation():\n",
    "    filename = 'Iris.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    learn_rate = 0.1 # DEFAULT\n",
    "    nb_iterations = 1000\n",
    "    data = data.sample(frac=1)\n",
    "    k = 3\n",
    "\n",
    "    X, y = divide_X_y(data.values)\n",
    "    X_normalizada = Normalizar_Datos(X)\n",
    "    data_normalizada  = np.concatenate((X_normalizada , y), axis=1)\n",
    "        \n",
    "    k_set = create_k_folds(pd.DataFrame(data_normalizada), k)\n",
    "   \n",
    "    W_array = [] \n",
    "    y_classes = np.unique(y)\n",
    "    accuracy_array_test = []\n",
    "    \n",
    "    for c in range(y_classes.shape[0]):\n",
    "        total_accuracy_test = 0\n",
    "        for i_test in range(0, k): \n",
    "            X_test = np.zeros ( (0, np.size(k_set[0]['X'])) )\n",
    "            y_test = np.zeros ( (0, np.size(k_set[0]['y'])) )\n",
    "            X_train = np.zeros( (0, (np.size(k_set[0]['X'])) * (y_classes.shape[0]-1)) )\n",
    "            y_train = np.zeros( (0, (np.size(k_set[0]['y'])) * (y_classes.shape[0]-1)) )\n",
    "     \n",
    "            for j in range(0, k): \n",
    "                if (i_test == j):\n",
    "                    X_test = k_set[i_test]['X']\n",
    "                    y_test = k_set[i_test]['y'] == y_classes[c]\n",
    "                    #y_train[ np.where( k_set[i_test]['y'] == y_classes[c]) ] = 1\n",
    "                else:\n",
    "                    X_train = k_set[j]['X']\n",
    "                    y_train = k_set[j]['y'] == y_classes[c]                   \n",
    "\n",
    "            y_train = np.reshape(y_train, y_train.shape[0])\n",
    "            y_test = np.reshape(y_test, y_test.shape[0])\n",
    "            \n",
    "            X_train = np.c_[X_train, np.ones(X_train.shape[0])]     #bias\n",
    "            X_test = np.c_[X_test, np.ones(X_test.shape[0])]        #bias\n",
    "          \n",
    "            theta = np.random.rand(np.size(X_train[0]))\n",
    "            X_train = X_train.astype(float)\n",
    "            y_train = y_train.astype(int)\n",
    "            W, cost_history = gradient_descent(X_train, y_train, theta, nb_iterations, learn_rate)\n",
    "            W_array.append(W)\n",
    "\n",
    "            X_test = X_test.astype('float')\n",
    "            y_test = y_test.astype(int)\n",
    "            accuracy_test = accuracy(X_test, y_test, W)\n",
    "            total_accuracy_test += accuracy_test\n",
    "            \n",
    "        total_accuracy_test = total_accuracy_test / k\n",
    "        accuracy_array_test.append(accuracy_test)\n",
    "\n",
    "    print('Iris-setosa, Iris-versicolor, Iris-virginica')\n",
    "    print(accuracy_array_test)\n",
    "    \n",
    "multiclass2_cross_validation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
